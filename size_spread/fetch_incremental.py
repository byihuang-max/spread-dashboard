#!/usr/bin/env python3
"""å¢é‡æ‹‰å– style_spread æ‰€éœ€çš„å…¨éƒ¨åŸå§‹æ—¥çº¿æ•°æ®ã€‚
å­˜å‚¨ï¼šCSV ä¸ºçœŸç›¸æºï¼ˆappend-onlyï¼‰ï¼ŒJSON cache ä¸ºå¿«é€Ÿè¯»å–å±‚ã€‚

CSV æ–‡ä»¶ï¼š
- ss_index_daily.csv  ï¼ˆæŒ‡æ•°æ—¥çº¿ï¼štrade_date, ts_code, close, pct_chgï¼‰
- ss_sw_daily.csv     ï¼ˆç”³ä¸‡è¡Œä¸šæ—¥çº¿ï¼štrade_date, ts_code, pct_change, amountï¼‰

JSON æ–‡ä»¶ï¼š
- style_spread_cache.json ï¼ˆä¿ç•™ï¼Œä¾› compute_spreads.py å¿«é€Ÿè¯»å–ï¼‰

é¦–æ¬¡è¿è¡Œï¼šå¦‚æœå·²æœ‰ JSON cache ä½†æ—  CSVï¼Œè‡ªåŠ¨ä» JSON è¿ç§»åˆ° CSVã€‚
åç»­è¿è¡Œï¼šæ¯ä¸ªä»£ç å„è‡ªä» CSV ä¸­æœ€åæ—¥æœŸçš„ä¸‹ä¸€å¤©å¼€å§‹æ‹‰å¢é‡ï¼ŒåŒæ—¶å†™ CSV + JSONã€‚
"""
import json, os, time, csv, requests
from datetime import datetime, timedelta
from collections import defaultdict

# === Tushare ===
TS_TOKEN = "33b3ff939d0d7954cd76cacce7cf6cbb2b3c3feda13d1ca2cfa594e20ecd"
TS_URL = "http://lianghua.nanyangqiankun.top"

BASE_DIR = os.path.dirname(os.path.abspath(__file__))
CACHE_PATH = os.path.join(BASE_DIR, "style_spread_cache.json")
IDX_CSV = os.path.join(BASE_DIR, "ss_index_daily.csv")
SW_CSV = os.path.join(BASE_DIR, "ss_sw_daily.csv")

IDX_HEADERS = ['trade_date', 'ts_code', 'close', 'pct_chg']
SW_HEADERS = ['trade_date', 'ts_code', 'pct_change', 'amount']

# æŒ‡æ•°ä»£ç ï¼ˆindex_daily æ¥å£ï¼‰
INDEX_CODES = {
    '000922.CSI': 'ä¸­è¯çº¢åˆ©', '000688.SH': 'ç§‘åˆ›50',
    '399303.SZ': 'å¾®ç›˜è‚¡', '000985.CSI': 'ä¸­è¯å…¨æŒ‡',
    '932000.CSI': 'ä¸­è¯2000', '000300.SH': 'æ²ªæ·±300',
    '399006.SZ': 'åˆ›ä¸šæ¿æŒ‡',
}

# ç”³ä¸‡è¡Œä¸šä»£ç ï¼ˆsw_daily æ¥å£ï¼‰
SW_CODES = {
    '801010.SI': 'å†œæ—ç‰§æ¸”', '801030.SI': 'åŸºç¡€åŒ–å·¥', '801040.SI': 'é’¢é“',
    '801050.SI': 'æœ‰è‰²é‡‘å±', '801080.SI': 'ç”µå­', '801880.SI': 'æ±½è½¦',
    '801110.SI': 'å®¶ç”¨ç”µå™¨', '801120.SI': 'é£Ÿå“é¥®æ–™', '801130.SI': 'çººç»‡æœé¥°',
    '801140.SI': 'è½»å·¥åˆ¶é€ ', '801150.SI': 'åŒ»è¯ç”Ÿç‰©', '801160.SI': 'å…¬ç”¨äº‹ä¸š',
    '801170.SI': 'äº¤é€šè¿è¾“', '801180.SI': 'æˆ¿åœ°äº§', '801200.SI': 'å•†è´¸é›¶å”®',
    '801210.SI': 'ç¤¾ä¼šæœåŠ¡', '801780.SI': 'é“¶è¡Œ', '801790.SI': 'éé“¶é‡‘è',
    '801230.SI': 'ç»¼åˆ', '801710.SI': 'å»ºç­‘ææ–™', '801720.SI': 'å»ºç­‘è£…é¥°',
    '801730.SI': 'ç”µåŠ›è®¾å¤‡', '801890.SI': 'æœºæ¢°è®¾å¤‡', '801740.SI': 'å›½é˜²å†›å·¥',
    '801750.SI': 'è®¡ç®—æœº', '801760.SI': 'ä¼ åª’', '801770.SI': 'é€šä¿¡',
    '801950.SI': 'ç…¤ç‚­', '801960.SI': 'çŸ³æ²¹çŸ³åŒ–', '801970.SI': 'ç¯ä¿',
    '801980.SI': 'ç¾å®¹æŠ¤ç†',
}


# â”€â”€ Tushare è¯·æ±‚ â”€â”€

def ts(api, params, fields=''):
    body = {"api_name": api, "token": TS_TOKEN, "params": params}
    if fields:
        body["fields"] = fields
    for attempt in range(3):
        try:
            r = requests.post(TS_URL, json=body, timeout=90)
            if not r.text.strip():
                print(f"  ç©ºå“åº”ï¼Œé‡è¯• {attempt+1}/3...")
                time.sleep(2)
                continue
            d = r.json()
            if d.get("data") and d["data"].get("fields") and d["data"].get("items"):
                return [dict(zip(d["data"]["fields"], row)) for row in d["data"]["items"]]
            return []
        except Exception as e:
            print(f"  è¯·æ±‚å¤±è´¥: {e}ï¼Œé‡è¯• {attempt+1}/3...")
            time.sleep(2)
    return []


# â”€â”€ CSV è¯»å†™ â”€â”€

def read_csv(path, headers):
    """è¯»å– CSVï¼Œè¿”å›è¡Œåˆ—è¡¨ [{col: val, ...}, ...]"""
    if not os.path.exists(path):
        return []
    with open(path, 'r', newline='', encoding='utf-8') as f:
        reader = csv.DictReader(f)
        return list(reader)


def write_csv(path, headers, rows):
    """å…¨é‡å†™å…¥ CSV"""
    with open(path, 'w', newline='', encoding='utf-8') as f:
        w = csv.DictWriter(f, fieldnames=headers)
        w.writeheader()
        w.writerows(rows)


def append_csv(path, headers, rows):
    """è¿½åŠ å†™å…¥ CSVï¼ˆå¦‚æœæ–‡ä»¶ä¸å­˜åœ¨åˆ™åˆ›å»ºå«è¡¨å¤´ï¼‰"""
    need_header = not os.path.exists(path) or os.path.getsize(path) == 0
    with open(path, 'a', newline='', encoding='utf-8') as f:
        w = csv.DictWriter(f, fieldnames=headers)
        if need_header:
            w.writeheader()
        w.writerows(rows)


def get_csv_last_dates(path, code_col='ts_code'):
    """ä» CSV è·å–æ¯ä¸ªä»£ç çš„æœ€åæ—¥æœŸ {code: 'YYYYMMDD'}"""
    rows = read_csv(path, [])
    last = {}
    for r in rows:
        code = r.get(code_col, '')
        dt = r.get('trade_date', '')
        if code and dt:
            if code not in last or dt > last[code]:
                last[code] = dt
    return last


# â”€â”€ JSON cache è¯»å†™ â”€â”€

def load_cache():
    if os.path.exists(CACHE_PATH):
        with open(CACHE_PATH, 'r', encoding='utf-8') as f:
            return json.load(f)
    return {"last_updated": "", "index_daily": {}, "sw_daily": {}}


def save_cache(cache):
    cache["last_updated"] = datetime.today().strftime("%Y%m%d")
    with open(CACHE_PATH, 'w', encoding='utf-8') as f:
        json.dump(cache, f, ensure_ascii=False, indent=2)


# â”€â”€ è¿ç§»ï¼šä»å·²æœ‰ JSON cache â†’ CSV â”€â”€

def migrate_json_to_csv(cache):
    """é¦–æ¬¡è¿ç§»ï¼šæŠŠ JSON cache ä¸­çš„æ•°æ®å¯¼å‡ºåˆ° CSV"""
    migrated = False

    # æŒ‡æ•°
    if not os.path.exists(IDX_CSV) and cache.get("index_daily"):
        print("  ğŸ“¦ ä» JSON cache è¿ç§»æŒ‡æ•°æ•°æ®åˆ° ss_index_daily.csv...")
        rows = []
        for code, entry in cache["index_daily"].items():
            for dt, vals in sorted(entry.get("data", {}).items()):
                rows.append({
                    'trade_date': dt,
                    'ts_code': code,
                    'close': vals.get('close', ''),
                    'pct_chg': vals.get('pct_chg', ''),
                })
        write_csv(IDX_CSV, IDX_HEADERS, rows)
        print(f"    ss_index_daily.csv: {len(rows)} è¡Œ")
        migrated = True

    # ç”³ä¸‡è¡Œä¸š
    if not os.path.exists(SW_CSV) and cache.get("sw_daily"):
        print("  ğŸ“¦ ä» JSON cache è¿ç§»ç”³ä¸‡è¡Œä¸šæ•°æ®åˆ° ss_sw_daily.csv...")
        rows = []
        for code, entry in cache["sw_daily"].items():
            for dt, vals in sorted(entry.get("data", {}).items()):
                rows.append({
                    'trade_date': dt,
                    'ts_code': code,
                    'pct_change': vals.get('pct_change', ''),
                    'amount': vals.get('amount', ''),
                })
        write_csv(SW_CSV, SW_HEADERS, rows)
        print(f"    ss_sw_daily.csv: {len(rows)} è¡Œ")
        migrated = True

    return migrated


# â”€â”€ å·¥å…·å‡½æ•° â”€â”€

def next_day(date_str):
    d = datetime.strptime(date_str, "%Y%m%d") + timedelta(days=1)
    return d.strftime("%Y%m%d")


def full_start():
    return (datetime.today() - timedelta(days=400)).strftime("%Y%m%d")


# â”€â”€ ä¸»æµç¨‹ â”€â”€

def fetch_incremental():
    cache = load_cache()
    today = datetime.today().strftime("%Y%m%d")

    # é¦–æ¬¡è¿ç§»
    migrate_json_to_csv(cache)

    # ä» CSV è·å–æ¯ä¸ªä»£ç çš„æœ€åæ—¥æœŸ
    idx_last = get_csv_last_dates(IDX_CSV)
    sw_last = get_csv_last_dates(SW_CSV)

    stats = {"index_new": 0, "index_skip": 0, "sw_new": 0, "sw_skip": 0}

    # --- æŒ‡æ•° ---
    print("=" * 50)
    print("æ‹‰å–æŒ‡æ•°æ•°æ®ï¼ˆindex_dailyï¼‰")
    print("=" * 50)
    for code, name in INDEX_CODES.items():
        last = idx_last.get(code)
        if last and last >= today:
            print(f"  {name}({code}): å·²æ˜¯æœ€æ–° ({last})ï¼Œè·³è¿‡")
            stats["index_skip"] += 1
            continue

        start = next_day(last) if last else full_start()
        print(f"  {name}({code}): ä» {start} æ‹‰å–...", end='', flush=True)

        data = ts('index_daily', {'ts_code': code, 'start_date': start, 'end_date': today},
                   fields='trade_date,close,pct_chg')

        # å†™å…¥ JSON cache
        if code not in cache.setdefault("index_daily", {}):
            cache["index_daily"][code] = {"name": name, "data": {}}

        csv_rows = []
        count = 0
        for item in (data or []):
            dt = item['trade_date']
            close = item.get('close')
            pct_chg = item.get('pct_chg')

            # JSON cache
            cache["index_daily"][code]["data"][dt] = {
                "close": close,
                "pct_chg": pct_chg,
            }
            # CSV row
            csv_rows.append({
                'trade_date': dt,
                'ts_code': code,
                'close': close if close is not None else '',
                'pct_chg': pct_chg if pct_chg is not None else '',
            })
            count += 1

        # è¿½åŠ  CSV
        if csv_rows:
            append_csv(IDX_CSV, IDX_HEADERS, csv_rows)

        total = len(cache["index_daily"][code]["data"])
        print(f" +{count}æ¡ (æ€»{total}æ¡)")
        stats["index_new"] += count

    # --- ç”³ä¸‡è¡Œä¸š ---
    print()
    print("=" * 50)
    print("æ‹‰å–ç”³ä¸‡è¡Œä¸šæ•°æ®ï¼ˆsw_dailyï¼‰")
    print("=" * 50)
    for code, name in SW_CODES.items():
        last = sw_last.get(code)
        if last and last >= today:
            print(f"  {name}({code}): å·²æ˜¯æœ€æ–° ({last})ï¼Œè·³è¿‡")
            stats["sw_skip"] += 1
            continue

        start = next_day(last) if last else full_start()
        print(f"  {name}({code}): ä» {start} æ‹‰å–...", end='', flush=True)

        data = ts('sw_daily', {'ts_code': code, 'start_date': start, 'end_date': today},
                   fields='trade_date,pct_change,amount')

        # å†™å…¥ JSON cache
        if code not in cache.setdefault("sw_daily", {}):
            cache["sw_daily"][code] = {"name": name, "data": {}}

        csv_rows = []
        count = 0
        for item in (data or []):
            dt = item['trade_date']
            pct_change = item.get('pct_change')
            amount = item.get('amount')

            # JSON cache
            cache["sw_daily"][code]["data"][dt] = {
                "pct_change": pct_change,
                "amount": amount,
            }
            # CSV row
            csv_rows.append({
                'trade_date': dt,
                'ts_code': code,
                'pct_change': pct_change if pct_change is not None else '',
                'amount': amount if amount is not None else '',
            })
            count += 1

        # è¿½åŠ  CSV
        if csv_rows:
            append_csv(SW_CSV, SW_HEADERS, csv_rows)

        total = len(cache["sw_daily"][code]["data"])
        print(f" +{count}æ¡ (æ€»{total}æ¡)")
        stats["sw_new"] += count

    # --- ä¿å­˜ JSON cache ---
    save_cache(cache)

    # --- ç»Ÿè®¡ ---
    cache_kb = os.path.getsize(CACHE_PATH) / 1024
    idx_csv_kb = os.path.getsize(IDX_CSV) / 1024 if os.path.exists(IDX_CSV) else 0
    sw_csv_kb = os.path.getsize(SW_CSV) / 1024 if os.path.exists(SW_CSV) else 0

    print()
    print("=" * 50)
    print("ç»Ÿè®¡æ‘˜è¦")
    print("=" * 50)
    print(f"  æŒ‡æ•°: æ–°å¢ {stats['index_new']} æ¡, è·³è¿‡ {stats['index_skip']} ä¸ª")
    print(f"  è¡Œä¸š: æ–°å¢ {stats['sw_new']} æ¡, è·³è¿‡ {stats['sw_skip']} ä¸ª")
    print(f"  æ–‡ä»¶å¤§å°:")
    print(f"    JSON cache: {cache_kb:.1f} KB")
    print(f"    ss_index_daily.csv: {idx_csv_kb:.1f} KB")
    print(f"    ss_sw_daily.csv: {sw_csv_kb:.1f} KB")

    # å„ä»£ç æ¡æ•°
    print()
    print("å„ä»£ç æ•°æ®æ¡æ•°:")
    for code in INDEX_CODES:
        entry = cache["index_daily"].get(code, {})
        n = len(entry.get("data", {}))
        print(f"  {INDEX_CODES[code]:8s} ({code}): {n}æ¡")
    for code in SW_CODES:
        entry = cache["sw_daily"].get(code, {})
        n = len(entry.get("data", {}))
        print(f"  {SW_CODES[code]:8s} ({code}): {n}æ¡")

    print(f"\nâœ… å®Œæˆï¼ˆCSV + JSON åŒå†™ï¼‰")


# â”€â”€ rebuild: ä» CSV é‡å»º JSON cache â”€â”€

def rebuild_cache_from_csv():
    """ä» CSV é‡å»º JSON cacheï¼ˆç¾éš¾æ¢å¤ç”¨ï¼‰"""
    print("ğŸ”„ ä» CSV é‡å»º JSON cache...")
    cache = {"last_updated": datetime.today().strftime("%Y%m%d"), "index_daily": {}, "sw_daily": {}}

    # æŒ‡æ•°
    idx_rows = read_csv(IDX_CSV, IDX_HEADERS)
    for r in idx_rows:
        code = r['ts_code']
        if code not in cache["index_daily"]:
            cache["index_daily"][code] = {"name": INDEX_CODES.get(code, code), "data": {}}
        close = r.get('close', '')
        pct_chg = r.get('pct_chg', '')
        cache["index_daily"][code]["data"][r['trade_date']] = {
            "close": float(close) if close else None,
            "pct_chg": float(pct_chg) if pct_chg else None,
        }
    print(f"  æŒ‡æ•°: {len(idx_rows)} è¡Œ")

    # ç”³ä¸‡
    sw_rows = read_csv(SW_CSV, SW_HEADERS)
    for r in sw_rows:
        code = r['ts_code']
        if code not in cache["sw_daily"]:
            cache["sw_daily"][code] = {"name": SW_CODES.get(code, code), "data": {}}
        pct_change = r.get('pct_change', '')
        amount = r.get('amount', '')
        cache["sw_daily"][code]["data"][r['trade_date']] = {
            "pct_change": float(pct_change) if pct_change else None,
            "amount": float(amount) if amount else None,
        }
    print(f"  è¡Œä¸š: {len(sw_rows)} è¡Œ")

    save_cache(cache)
    print(f"  âœ… JSON cache å·²é‡å»º ({os.path.getsize(CACHE_PATH)/1024:.1f} KB)")


if __name__ == "__main__":
    import sys
    t0 = time.time()

    if '--rebuild' in sys.argv:
        rebuild_cache_from_csv()
    else:
        fetch_incremental()

    print(f"è€—æ—¶: {time.time()-t0:.1f}ç§’")
