#!/usr/bin/env python3
"""
æœŸæƒæƒ…ç»ªé¢æ¿ - è®¡ç®—
1. BSæ¨¡å‹åç®—IV
2. IVæœŸé™ç»“æ„ + 20æ—¥æ»šåŠ¨åˆ†ä½
3. PCR(OI) + OIåˆ†å¸ƒ
4. å¼‚å¸¸æ£€æµ‹ï¼ˆIVçªå˜ + OIæ¿€å¢ï¼‰
"""
import os, json, math
import pandas as pd
import numpy as np
from scipy.stats import norm
from scipy.optimize import brentq

SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))
CACHE_DIR = os.path.join(SCRIPT_DIR, 'cache')
OUTPUT_JSON = os.path.join(SCRIPT_DIR, 'option_sentiment.json')

RISK_FREE = 0.015  # æ— é£é™©åˆ©ç‡ 1.5%

# æ ‡çš„æ˜ å°„
UNDERLYING_MAP = {
    'OP000300.SH': {'name': 'æ²ªæ·±300',    'etf': '000300.SH'},
    'OP000016.SH': {'name': 'ä¸Šè¯50',     'etf': '000016.SH'},
    'OP510500.SH': {'name': 'ä¸­è¯500ETF', 'etf': '510500.SH'},
    'OP000852.SH': {'name': 'ä¸­è¯1000',   'etf': '000852.SH'},
}


def load_csv(name):
    path = os.path.join(CACHE_DIR, name)
    if not os.path.exists(path):
        return pd.DataFrame()
    return pd.read_csv(path)


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Black-Scholes
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
def bs_price(S, K, T, r, sigma, cp='C'):
    if T <= 0 or sigma <= 0:
        return max(S - K, 0) if cp == 'C' else max(K - S, 0)
    d1 = (math.log(S / K) + (r + 0.5 * sigma ** 2) * T) / (sigma * math.sqrt(T))
    d2 = d1 - sigma * math.sqrt(T)
    if cp == 'C':
        return S * norm.cdf(d1) - K * math.exp(-r * T) * norm.cdf(d2)
    else:
        return K * math.exp(-r * T) * norm.cdf(-d2) - S * norm.cdf(-d1)


def implied_vol(market_price, S, K, T, r, cp='C'):
    if T <= 1e-6:
        return None
    intrinsic = max(S - K, 0) if cp == 'C' else max(K - S, 0)
    if market_price <= intrinsic + 1e-6:
        return None
    try:
        iv = brentq(lambda sig: bs_price(S, K, T, r, sig, cp) - market_price,
                     0.01, 5.0, xtol=1e-6, maxiter=100)
        return iv if 0.01 < iv < 5.0 else None
    except:
        return None


def calc_option_sentiment():
    contracts = load_csv('opt_contracts.csv')
    opt_daily = load_csv('opt_daily.csv')
    etf_daily = load_csv('etf_daily.csv')

    if contracts.empty or opt_daily.empty or etf_daily.empty:
        print("æ•°æ®ä¸è¶³!")
        return

    # ç±»å‹è½¬æ¢
    for col in ['exercise_price']:
        if col in contracts.columns:
            contracts[col] = pd.to_numeric(contracts[col], errors='coerce')
    for col in ['close', 'settle', 'vol', 'amount', 'oi']:
        if col in opt_daily.columns:
            opt_daily[col] = pd.to_numeric(opt_daily[col], errors='coerce')
    etf_daily['close'] = pd.to_numeric(etf_daily['close'], errors='coerce')

    # åˆå¹¶åˆçº¦ä¿¡æ¯åˆ°æ—¥çº¿
    merged = opt_daily.merge(
        contracts[['ts_code', 'opt_code', 'call_put', 'exercise_price', 'maturity_date', 'underlying', 'underlying_name']],
        on='ts_code', how='inner'
    )

    # è·å–æ¯å¤©æ¯ä¸ªETFçš„æ”¶ç›˜ä»·
    etf_prices = {}
    for _, row in etf_daily.iterrows():
        etf_prices[(str(row['ts_code']), str(row['trade_date']))] = float(row['close'])

    # æ„å»º underlying -> etf æ˜ å°„
    und_etf = {k: v['etf'] for k, v in UNDERLYING_MAP.items()}

    result = {
        'update_time': pd.Timestamp.now().strftime('%Y-%m-%d %H:%M'),
        'underlyings': [],
    }

    trade_dates = sorted(merged['trade_date'].unique())
    latest_date = str(trade_dates[-1])

    for und_code, und_info in UNDERLYING_MAP.items():
        print(f"\nå¤„ç† {und_info['name']}...")
        etf_code = und_info['etf']
        und_data = merged[merged['underlying'] == und_code].copy()
        if und_data.empty:
            continue

        # â”€â”€ è®¡ç®—IV â”€â”€
        iv_records = []
        for _, row in und_data.iterrows():
            td = str(row['trade_date'])
            # å¤„ç†æ—¥æœŸæ ¼å¼
            td_key = td.replace('-', '') if '-' in td else td
            S = etf_prices.get((etf_code, td_key))
            if S is None:
                # å°è¯•å¸¦æ¨ªçº¿çš„æ ¼å¼
                td_dash = f"{td_key[:4]}-{td_key[4:6]}-{td_key[6:8]}" if len(td_key) == 8 else td_key
                S = etf_prices.get((etf_code, td_dash))
            if S is None or S <= 0:
                continue

            K = float(row['exercise_price'])
            mat = str(row['maturity_date']).replace('-', '')
            if len(mat) != 8:
                continue
            try:
                mat_date = pd.Timestamp(mat)
                td_date = pd.Timestamp(td_key if len(td_key) == 8 else td)
            except:
                continue
            T = (mat_date - td_date).days / 365.0
            if T <= 0:
                continue

            price = float(row['settle']) if pd.notna(row['settle']) and row['settle'] > 0 else float(row['close'])
            if price <= 0:
                continue

            cp = str(row['call_put'])
            iv = implied_vol(price, S, K, T, RISK_FREE, cp)

            iv_records.append({
                'ts_code': row['ts_code'],
                'trade_date': td_key,
                'call_put': cp,
                'exercise_price': K,
                'maturity_date': mat,
                'S': S, 'T': T,
                'settle': price,
                'vol': float(row['vol']) if pd.notna(row['vol']) else 0,
                'oi': float(row['oi']) if pd.notna(row['oi']) else 0,
                'iv': iv,
                'moneyness': S / K,  # >1 å®å€¼Call/è™šå€¼Put
            })

        if not iv_records:
            print(f"  æ— æœ‰æ•ˆIVæ•°æ®")
            continue

        iv_df = pd.DataFrame(iv_records)
        iv_df = iv_df[iv_df['iv'].notna()].copy()
        print(f"  æœ‰æ•ˆIVè®°å½•: {len(iv_df)}")

        # â”€â”€ å¹³å€¼é™„è¿‘åˆçº¦ï¼ˆmoneyness 0.95~1.05ï¼‰â”€â”€
        atm = iv_df[(iv_df['moneyness'] >= 0.95) & (iv_df['moneyness'] <= 1.05)].copy()

        # â”€â”€ 1. IVæœŸé™ç»“æ„ï¼ˆæœ€æ–°ä¸€å¤©ï¼‰â”€â”€
        latest_atm = atm[atm['trade_date'] == latest_date.replace('-', '')]
        if latest_atm.empty:
            # å°è¯•æœ€è¿‘æœ‰æ•°æ®çš„æ—¥æœŸ
            avail = sorted(atm['trade_date'].unique())
            if avail:
                latest_date_use = avail[-1]
                latest_atm = atm[atm['trade_date'] == latest_date_use]
            else:
                latest_date_use = latest_date
        else:
            latest_date_use = latest_date.replace('-', '')

        term_structure = []
        if not latest_atm.empty:
            for mat, grp in latest_atm.groupby('maturity_date'):
                avg_iv = grp['iv'].median()
                T = grp['T'].iloc[0]
                mat_label = f"{mat[4:6]}-{mat[6:8]}"
                term_structure.append({
                    'maturity': mat_label,
                    'maturity_date': mat,
                    'T': round(T, 4),
                    'iv': round(float(avg_iv) * 100, 2),  # è½¬ç™¾åˆ†æ¯”
                })
            term_structure.sort(key=lambda x: x['maturity_date'])

        # â”€â”€ 2. IV 20æ—¥æ»šåŠ¨åˆ†ä½ â”€â”€
        # å–è¿‘æœˆå¹³å€¼Callçš„IVæ—¶é—´åºåˆ—
        maturities = sorted(atm['maturity_date'].unique())
        near_mat = maturities[0] if maturities else None
        iv_history = []
        if near_mat:
            near_atm = atm[(atm['maturity_date'] == near_mat) & (atm['call_put'] == 'C')]
            daily_iv = near_atm.groupby('trade_date')['iv'].median().sort_index()
            for td, iv_val in daily_iv.items():
                # 20æ—¥æ»šåŠ¨åˆ†ä½
                idx = daily_iv.index.get_loc(td)
                window = daily_iv.iloc[max(0, idx - 19):idx + 1]
                if len(window) >= 5:
                    pct = (window < iv_val).sum() / len(window)
                else:
                    pct = 0.5
                td_str = str(td)
                iv_history.append({
                    'date': f"{td_str[4:6]}-{td_str[6:8]}" if len(td_str) == 8 else td_str,
                    'iv': round(float(iv_val) * 100, 2),
                    'percentile': round(float(pct) * 100, 1),
                })

        # â”€â”€ 3. PCR (by OI) â”€â”€
        pcr_history = []
        for td in sorted(iv_df['trade_date'].unique()):
            day = iv_df[iv_df['trade_date'] == td]
            put_oi = day[day['call_put'] == 'P']['oi'].sum()
            call_oi = day[day['call_put'] == 'C']['oi'].sum()
            pcr = put_oi / call_oi if call_oi > 0 else 0
            put_vol = day[day['call_put'] == 'P']['vol'].sum()
            call_vol = day[day['call_put'] == 'C']['vol'].sum()
            pcr_vol = put_vol / call_vol if call_vol > 0 else 0
            td_str = str(td)
            pcr_history.append({
                'date': f"{td_str[4:6]}-{td_str[6:8]}" if len(td_str) == 8 else td_str,
                'pcr_oi': round(float(pcr), 3),
                'pcr_vol': round(float(pcr_vol), 3),
            })

        # â”€â”€ 4. OI æŒ‰è¡Œæƒä»·åˆ†å¸ƒï¼ˆæœ€æ–°ä¸€å¤©ï¼‰â”€â”€
        latest_all = iv_df[iv_df['trade_date'] == latest_date_use]
        # å–è¿‘æœˆ
        if near_mat:
            latest_near = latest_all[latest_all['maturity_date'] == near_mat]
        else:
            latest_near = latest_all
        
        oi_dist = []
        if not latest_near.empty:
            S_latest = latest_near['S'].iloc[0]
            for K, grp in latest_near.groupby('exercise_price'):
                call_oi = grp[grp['call_put'] == 'C']['oi'].sum()
                put_oi = grp[grp['call_put'] == 'P']['oi'].sum()
                call_iv = grp[grp['call_put'] == 'C']['iv'].median()
                put_iv = grp[grp['call_put'] == 'P']['iv'].median()
                oi_dist.append({
                    'strike': float(K),
                    'call_oi': int(call_oi),
                    'put_oi': int(put_oi),
                    'call_iv': round(float(call_iv) * 100, 2) if pd.notna(call_iv) else None,
                    'put_iv': round(float(put_iv) * 100, 2) if pd.notna(put_iv) else None,
                })
            oi_dist.sort(key=lambda x: x['strike'])

        # â”€â”€ 5. å¼‚å¸¸æ£€æµ‹ â”€â”€
        anomalies = []
        # OI å¼‚å¸¸ï¼šæœ€æ–°ä¸€å¤© vs å‰5å¤©å‡å€¼
        dates_sorted = sorted(iv_df['trade_date'].unique())
        if len(dates_sorted) >= 6:
            prev_dates = dates_sorted[-6:-1]
            prev_data = iv_df[iv_df['trade_date'].isin(prev_dates)]
            prev_oi_avg = prev_data.groupby(['ts_code'])['oi'].mean()
            
            latest_data = iv_df[iv_df['trade_date'] == latest_date_use]
            for _, row in latest_data.iterrows():
                prev_avg = prev_oi_avg.get(row['ts_code'], 0)
                if prev_avg > 50 and row['oi'] > prev_avg * 2:
                    anomalies.append({
                        'type': 'oi_surge',
                        'label': f"{'P' if row['call_put']=='P' else 'C'}{row['exercise_price']:.2f}",
                        'detail': f"OI {int(row['oi'])} vs å‡å€¼{int(prev_avg)}ï¼ˆ{row['oi']/prev_avg:.1f}xï¼‰",
                        'call_put': row['call_put'],
                        'strike': float(row['exercise_price']),
                        'oi': int(row['oi']),
                        'prev_avg': int(prev_avg),
                    })

            # IV å¼‚å¸¸ï¼šå½“æ—¥IV vs å‰5æ—¥çš„å˜åŒ–
            if not atm.empty:
                for mat in atm['maturity_date'].unique():
                    mat_atm = atm[atm['maturity_date'] == mat]
                    for cp in ['C', 'P']:
                        cp_data = mat_atm[mat_atm['call_put'] == cp]
                        daily_med = cp_data.groupby('trade_date')['iv'].median().sort_index()
                        if len(daily_med) >= 6:
                            prev_ivs = daily_med.iloc[-6:-1]
                            latest_iv = daily_med.iloc[-1]
                            mean_iv = prev_ivs.mean()
                            std_iv = prev_ivs.std()
                            if std_iv > 0 and abs(latest_iv - mean_iv) > 2 * std_iv:
                                direction = 'æ”¾å¤§' if latest_iv > mean_iv else 'ç¼©å°'
                                mat_label = f"{str(mat)[4:6]}æœˆ"
                                anomalies.append({
                                    'type': 'iv_spike',
                                    'label': f"{mat_label}{cp} IV{direction}",
                                    'detail': f"IV {latest_iv*100:.1f}% vs å‡å€¼{mean_iv*100:.1f}%ï¼ˆ{abs(latest_iv-mean_iv)/std_iv:.1f}Ïƒï¼‰",
                                    'direction': direction,
                                })

        # â”€â”€ æ±‡æ€» â”€â”€
        latest_pcr = pcr_history[-1] if pcr_history else {}
        latest_iv_val = iv_history[-1] if iv_history else {}

        und_result = {
            'code': und_code,
            'name': und_info['name'],
            'latest_date': latest_date_use,
            'term_structure': term_structure,
            'iv_history': iv_history,
            'pcr_history': pcr_history,
            'oi_distribution': oi_dist,
            'anomalies': anomalies,
            'summary': {
                'pcr_oi': latest_pcr.get('pcr_oi', 0),
                'pcr_vol': latest_pcr.get('pcr_vol', 0),
                'atm_iv': latest_iv_val.get('iv', 0),
                'iv_percentile': latest_iv_val.get('percentile', 50),
                'spot': float(latest_near['S'].iloc[0]) if not latest_near.empty else 0,
                'anomaly_count': len(anomalies),
            },
        }
        result['underlyings'].append(und_result)
        print(f"  PCR(OI)={latest_pcr.get('pcr_oi',0)}, ATM IV={latest_iv_val.get('iv',0)}%, å¼‚å¸¸={len(anomalies)}")

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # å…¨å±€ä¿¡å·æ±‡æ€»
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    global_signals = []
    for u in result['underlyings']:
        s = u['summary']
        name = u['name']

        # PCR æç«¯
        if s['pcr_oi'] > 1.3:
            global_signals.append(f"{name} PCR(OI) {s['pcr_oi']:.2f}ï¼Œçœ‹ç©ºæŒä»“æé‡ ğŸ”´")
        elif s['pcr_oi'] < 0.5:
            global_signals.append(f"{name} PCR(OI) {s['pcr_oi']:.2f}ï¼Œçœ‹å¤šæƒ…ç»ªäº¢å¥‹ âš ï¸")

        # IV åˆ†ä½æç«¯
        if s['iv_percentile'] <= 10:
            global_signals.append(f"{name} IVåˆ†ä½ {s['iv_percentile']:.0f}%ï¼Œæ³¢åŠ¨ç‡æä½ï¼Œå¯¹å†²æˆæœ¬ä¾¿å®œ ğŸŸ¢")
        elif s['iv_percentile'] >= 90:
            global_signals.append(f"{name} IVåˆ†ä½ {s['iv_percentile']:.0f}%ï¼Œæ³¢åŠ¨ç‡æé«˜ï¼Œå¸‚åœºææ…Œ ğŸ”´")

        # IV æœŸé™ç»“æ„å€’æŒ‚
        ts = u.get('term_structure', [])
        if len(ts) >= 2 and ts[0]['iv'] > ts[1]['iv'] * 1.05:
            global_signals.append(f"{name} IVæœŸé™å€’æŒ‚ï¼ˆè¿‘æœˆ{ts[0]['iv']:.1f}% > æ¬¡æœˆ{ts[1]['iv']:.1f}%ï¼‰âš ï¸")

        # OIæ¿€å¢çš„å¤§å¤´
        oi_surges = [a for a in u.get('anomalies', []) if a['type'] == 'oi_surge']
        if oi_surges:
            biggest = max(oi_surges, key=lambda x: x.get('oi', 0))
            ratio = biggest['oi'] / biggest['prev_avg'] if biggest.get('prev_avg', 0) > 0 else 0
            global_signals.append(f"{name} {biggest['label']} OIæ¿€å¢{ratio:.1f}xï¼Œå¤§èµ„é‡‘å»ºä»“ ğŸ”¶")

    result['global_signals'] = global_signals if global_signals else ['å½“å‰æœŸæƒå¸‚åœºæ— æç«¯ä¿¡å· âœ…']

    with open(OUTPUT_JSON, 'w', encoding='utf-8') as f:
        json.dump(result, f, ensure_ascii=False, indent=2)
    print(f"\nè¾“å‡º: {OUTPUT_JSON}")


if __name__ == '__main__':
    calc_option_sentiment()
