#!/usr/bin/env python3
"""
è¶…é¢ç¯å¢ƒå›æº¯å½’å› 
================
å¯¹äº§å“å‡€å€¼çš„æ¯ä¸ªå‘¨é¢‘ç‚¹ï¼Œå›æº¯è®¡ç®—å½“æ—¶çš„ç¯å¢ƒå› å­çŠ¶æ€ã€‚
ç”¨äºå‰ç«¯å›¾è¡¨çš„hover tooltipå½’å› è§£é‡Šã€‚

è¾“å‡ºï¼šexcess_attribution.json
"""
import json, os, math, statistics
from datetime import datetime, timedelta

BASE = os.path.dirname(os.path.abspath(__file__))
DASH = os.path.dirname(os.path.dirname(BASE))

def load(path):
    with open(path, 'r') as f:
        return json.load(f)

def safe_load(path, default=None):
    try:
        return load(path)
    except:
        return default

# â•â•â• æ•°æ®åŠ è½½ â•â•â•
qs_data = load(os.path.join(BASE, 'quant_stock_data.json'))
fund_nav = load(os.path.join(DASH, 'size_spread', 'fund_nav', 'fund_nav_quant-stock.json'))
cross_vol_hist = safe_load(os.path.join(BASE, 'cross_vol_history.json'), {})

chart = fund_nav['fund']['chart']
nav_dates = chart['dates']       # ['2025-01-03', ...]  å‘¨é¢‘
nav_excess = chart['excess']     # ç´¯è®¡è¶…é¢åºåˆ—

# æŠŠæ—¥é¢‘æ•°æ®æŒ‰ trade_date ç´¢å¼•æ–¹ä¾¿æŸ¥æ‰¾
def build_index(arr, date_key='date'):
    """æ—¥é¢‘æ•°ç»„ â†’ {date_str: record}"""
    idx = {}
    for r in arr:
        idx[r[date_key]] = r
    return idx

amount_idx = build_index(qs_data['total_amount'])
share_idx = build_index(qs_data['index_share'])
basis_idx = build_index(qs_data['basis'])
factor_idx = build_index(qs_data['factor'])

# æˆªé¢æ³¢åŠ¨ç‡å†å²
cv_data = cross_vol_hist.get('data', [])
cv_idx = {}
for r in cv_data:
    cv_idx[r['date']] = r

# åŸºå·®å†å²åºåˆ—ï¼ˆç”¨äºç®—åˆ†ä½æ•°ï¼‰
all_im = [d.get('IM', 0) for d in qs_data['basis']]

# æŒ‡æ•°æ—¥çº¿ï¼ˆç”¨äºè™¹å¸éªŒè¯ï¼‰
import csv
def load_index_csv():
    path = os.path.join(BASE, 'qs_index_daily.csv')
    data = {}
    try:
        with open(path, 'r') as f:
            reader = csv.DictReader(f)
            for row in reader:
                code = row.get('ts_code', '')
                if not code: continue
                if code not in data: data[code] = []
                data[code].append({
                    'date': row.get('trade_date', ''),
                    'close': float(row.get('close', 0)),
                    'amount': float(row.get('amount', 0)) if row.get('amount') else 0,
                })
        for code in data:
            data[code].sort(key=lambda x: x['date'])
    except:
        pass
    return data

idx_daily = load_index_csv()
# æŒ‰æ—¥æœŸç´¢å¼•
idx_daily_map = {}
for code, series in idx_daily.items():
    idx_daily_map[code] = {r['date']: r for r in series}

def date_to_yyyymmdd(d):
    """'2025-01-03' â†’ '20250103'"""
    return d.replace('-', '')

def get_nearby_dates(target_yyyymmdd, n_before=5, n_after=0):
    """è·å–targeté™„è¿‘çš„äº¤æ˜“æ—¥åˆ—è¡¨ï¼ˆä»æˆäº¤é¢æ•°æ®ä¸­æŸ¥ï¼‰"""
    all_dates = sorted(amount_idx.keys())
    try:
        pos = all_dates.index(target_yyyymmdd)
    except ValueError:
        # æ‰¾æœ€è¿‘çš„
        for i, d in enumerate(all_dates):
            if d >= target_yyyymmdd:
                pos = i
                break
        else:
            pos = len(all_dates) - 1
    start = max(0, pos - n_before + 1)
    end = min(len(all_dates), pos + n_after + 1)
    return all_dates[start:end]


def calc_env_at(date_str):
    """
    è®¡ç®—æŸå‘¨é¢‘æ—¥æœŸçš„ç¯å¢ƒå› å­å¿«ç…§ã€‚
    è¿”å›å„å› å­çš„å…³é”®æŒ‡æ ‡ã€‚
    """
    td = date_to_yyyymmdd(date_str)
    result = {}

    # â”€â”€ æµåŠ¨æ€§ â”€â”€
    nearby = get_nearby_dates(td, n_before=20)
    amounts = []
    for d in nearby:
        r = amount_idx.get(d)
        if r:
            amounts.append(r.get('amount_yi', r.get('amount', 0)))

    if amounts:
        recent5 = amounts[-5:] if len(amounts) >= 5 else amounts
        ma5 = sum(recent5) / len(recent5)
        ma20 = sum(amounts) / len(amounts)
        latest_amt = amounts[-1]
        cv = (statistics.stdev(amounts) / ma20) if len(amounts) > 1 and ma20 > 0 else 0
        trend = 'æ”¾é‡' if ma5 > ma20 * 1.05 else ('ç¼©é‡' if ma5 < ma20 * 0.95 else 'å¹³ç¨³')
        result['liquidity'] = {
            'amount': round(latest_amt, 0),
            'ma5': round(ma5, 0),
            'ma20': round(ma20, 0),
            'cv': round(cv, 3),
            'trend': trend,
        }

    # â”€â”€ é£æ ¼é›†ä¸­åº¦ + è¿ç§» â”€â”€
    style_keys = ['æ²ªæ·±300', 'ä¸­è¯500', 'ä¸­è¯1000', 'ä¸­è¯2000', 'ç§‘åˆ›+åˆ›ä¸šæ¿']
    share_dates_20 = get_nearby_dates(td, n_before=20)
    share_dates_5 = share_dates_20[-5:] if len(share_dates_20) >= 5 else share_dates_20

    shares_5 = [share_idx[d] for d in share_dates_5 if d in share_idx]
    shares_20 = [share_idx[d] for d in share_dates_20 if d in share_idx]

    if shares_5:
        avg5 = {}
        avg20 = {}
        for k in style_keys:
            v5 = [s.get(k, 0) for s in shares_5]
            avg5[k] = sum(v5) / len(v5)
            v20 = [s.get(k, 0) for s in shares_20]
            avg20[k] = sum(v20) / len(v20) if v20 else avg5[k]

        total = sum(avg5.values())
        if total > 0:
            norm = {k: v / total for k, v in avg5.items()}
            hhi = sum(v ** 2 for v in norm.values())
        else:
            hhi = 0.2

        # è¿ç§»
        deltas = {k: round(avg5[k] - avg20[k], 2) for k in style_keys}
        max_mover = max(deltas, key=lambda k: abs(deltas[k]))
        max_delta = deltas[max_mover]

        dominant = max(avg5, key=avg5.get)

        result['style'] = {
            'hhi': round(hhi, 4),
            'dominant': dominant,
            'dominant_pct': round(avg5[dominant], 1),
            'max_mover': max_mover,
            'max_delta': max_delta,
            'deltas': deltas,
        }

    # â”€â”€ åŸºå·® + è™¹å¸ â”€â”€
    basis_r = basis_idx.get(td)
    # æ‰¾æœ€è¿‘çš„äº¤æ˜“æ—¥
    if not basis_r:
        for d in reversed(get_nearby_dates(td, n_before=5)):
            if d in basis_idx:
                basis_r = basis_idx[d]
                break

    if basis_r:
        im = basis_r.get('IM', 0)
        ic = basis_r.get('IC', 0)
        if_b = basis_r.get('IF', 0)

        # å†å²åˆ†ä½ï¼ˆæˆªæ­¢åˆ°å½“å‰æ—¥æœŸï¼‰
        hist_im = [d.get('IM', 0) for d in qs_data['basis'] if d['date'] <= td]
        rank = sum(1 for x in hist_im if x <= im)
        pctile = rank / len(hist_im) * 100 if hist_im else 50

        # è™¹å¸æ£€æŸ¥
        siphon = False
        siphon_note = ''
        is_premium = im > 0 or pctile > 80

        if is_premium and shares_5 and shares_20:
            big5 = sum(s.get('æ²ªæ·±300', 0) + s.get('ä¸­è¯500', 0) for s in shares_5) / len(shares_5)
            big20 = sum(s.get('æ²ªæ·±300', 0) + s.get('ä¸­è¯500', 0) for s in shares_20) / len(shares_20)
            surge = big5 - big20

            # å¤§ç¥¨è·‘èµ¢å°ç¥¨
            ret300 = ret1000 = None
            s300 = idx_daily_map.get('000300.SH', {})
            s1000 = idx_daily_map.get('000852.SH', {})
            dates_5 = get_nearby_dates(td, n_before=5)
            if len(dates_5) >= 2:
                d_end = dates_5[-1]
                d_start = dates_5[0]
                if d_end in s300 and d_start in s300:
                    ret300 = (s300[d_end]['close'] / s300[d_start]['close'] - 1) * 100
                if d_end in s1000 and d_start in s1000:
                    ret1000 = (s1000[d_end]['close'] / s1000[d_start]['close'] - 1) * 100

            outperf = (ret300 - ret1000) if (ret300 is not None and ret1000 is not None) else None
            cond_a = surge > 2.0
            cond_b = outperf is not None and outperf > 2.0
            siphon = cond_a and cond_b

            if siphon:
                siphon_note = f'å æ¯”+{surge:.1f}pp, 300è·‘èµ¢{outperf:.1f}%'

        result['basis'] = {
            'im': round(im, 2),
            'ic': round(ic, 2),
            'if': round(if_b, 2),
            'pctile': round(pctile, 0),
            'siphon': siphon,
            'siphon_note': siphon_note,
        }

    # â”€â”€ ç¦»æ•£åº¦ï¼ˆå¦‚æœæœ‰å†å²æ•°æ®ï¼‰â”€â”€
    cv_r = cv_idx.get(td)
    if not cv_r:
        for d in reversed(get_nearby_dates(td, n_before=3)):
            d8 = d  # already yyyymmdd
            if d8 in cv_idx:
                cv_r = cv_idx[d8]
                break
    if cv_r:
        result['dispersion'] = {
            'cross_vol': cv_r.get('cross_vol', 0),
            'stock_count': cv_r.get('stock_count', 0),
        }

    # â”€â”€ å› å­åŠ¨æ€ â”€â”€
    factor_dates = get_nearby_dates(td, n_before=10)
    early = factor_dates[:5]
    late = factor_dates[-5:]
    fnames = qs_data.get('factor_names', [])
    factor_chgs = {}
    for fn in fnames:
        e_vals = [factor_idx[d].get(fn, 1) for d in early if d in factor_idx]
        l_vals = [factor_idx[d].get(fn, 1) for d in late if d in factor_idx]
        if e_vals and l_vals:
            e_avg = sum(e_vals) / len(e_vals)
            l_avg = sum(l_vals) / len(l_vals)
            chg = (l_avg / e_avg - 1) * 100 if e_avg else 0
            factor_chgs[fn] = round(chg, 2)
    if factor_chgs:
        result['factors'] = factor_chgs

    return result


# â•â•â• Main â•â•â•
if __name__ == '__main__':
    print('ğŸ“Š è¶…é¢ç¯å¢ƒå›æº¯å½’å› ')
    print(f'  äº§å“å‡€å€¼: {len(nav_dates)}ä¸ªå‘¨é¢‘ç‚¹')
    print(f'  æˆªé¢æ³¢åŠ¨ç‡å†å²: {len(cv_data)}å¤©')

    # è®¡ç®—æ¯ä¸ªå‘¨é¢‘ç‚¹çš„ç¯å¢ƒå¿«ç…§
    points = []
    for i in range(len(nav_dates)):
        date = nav_dates[i]
        ex = nav_excess[i]
        delta = (nav_excess[i] - nav_excess[i-1]) if i > 0 else 0

        # ç¯å¢ƒæ ‡ç­¾
        if delta < -0.01:
            zone = 'bad'       # æ˜æ˜¾å›æ’¤
        elif delta < -0.003:
            zone = 'weak'      # å°å›æ’¤/èµ°å¹³
        elif delta > 0.01:
            zone = 'good'      # é¡ºé£
        else:
            zone = 'neutral'

        env = calc_env_at(date)

        points.append({
            'date': date,
            'excess': round(ex, 6),
            'delta': round(delta, 6),
            'zone': zone,
            'env': env,
        })

        if i % 10 == 0:
            print(f'  {date} excess={ex:.4f} Î”={delta:+.4f} [{zone}]')

    # è¯†åˆ«è¿ç»­éš¾åšæ—¶æ®µ
    bad_periods = []
    streak_start = None
    for i, p in enumerate(points):
        if p['zone'] in ('bad', 'weak'):
            if streak_start is None:
                streak_start = i
        else:
            if streak_start is not None:
                # ç»“æŸä¸€æ®µ
                dd = points[streak_start]['excess'] - points[i-1]['excess']
                bad_periods.append({
                    'start': points[streak_start]['date'],
                    'end': points[i-1]['date'],
                    'start_idx': streak_start,
                    'end_idx': i-1,
                    'drawdown': round(dd, 6),
                    'weeks': i - streak_start,
                })
                streak_start = None

    if streak_start is not None:
        dd = points[streak_start]['excess'] - points[-1]['excess']
        bad_periods.append({
            'start': points[streak_start]['date'],
            'end': points[-1]['date'],
            'start_idx': streak_start,
            'end_idx': len(points)-1,
            'drawdown': round(dd, 6),
            'weeks': len(points) - streak_start,
        })

    output = {
        'update_time': datetime.now().strftime('%Y-%m-%d %H:%M'),
        'points': points,
        'bad_periods': bad_periods,
        'nav_dates': nav_dates,
        'nav_excess': [round(x, 6) for x in nav_excess],
        'fund_nav': chart.get('fund_nav', []),
        'index_nav': chart.get('index_nav', []),
    }

    out_path = os.path.join(BASE, 'excess_attribution.json')
    with open(out_path, 'w', encoding='utf-8') as f:
        json.dump(output, f, ensure_ascii=False, indent=2)

    print(f'\nâœ… è¾“å‡º {len(points)} ä¸ªç‚¹ â†’ {out_path}')
    print(f'  éš¾åšæ—¶æ®µ: {len(bad_periods)} æ®µ')
    for bp in bad_periods:
        print(f'    {bp["start"]} ~ {bp["end"]} ({bp["weeks"]}å‘¨) è¶…é¢{bp["drawdown"]:+.4f}')
