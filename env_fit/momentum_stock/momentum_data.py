#!/usr/bin/env python3
"""
å¼ºåŠ¿è‚¡æƒ…ç»ªæŒ‡æ ‡æ•°æ®è„šæœ¬ï¼ˆCSVå¢é‡æ¨¡å¼ï¼‰
ä» Tushare limit_list_d æ‹‰å–æ¶¨è·Œåœæ•°æ®ï¼Œå¢é‡è¿½åŠ åˆ° CSV
CSV åŒ…å«ï¼šåŸºç¡€æ•°æ®åˆ— + è®¡ç®—æŒ‡æ ‡åˆ— + è®¡ç®—å…¬å¼åˆ—

å¢é‡ç­–ç•¥ï¼š
- momentum_raw.csv å­˜åŸºç¡€æ•°æ®ï¼ˆæ¯æ—¥æ¶¨è·ŒåœåŸå§‹ç»Ÿè®¡ï¼‰
- momentum_sentiment.csv å­˜å®Œæ•´æ•°æ®ï¼ˆåŸºç¡€+è®¡ç®—æŒ‡æ ‡+å…¬å¼ï¼‰
- æ¯æ¬¡åªæ‹‰ CSV ä¸­æ²¡æœ‰çš„æ–°æ—¥æœŸ
- æœ€åä»è¾“å‡º momentum_sentiment.jsonï¼ˆä¾› inject_momentum.py ä½¿ç”¨ï¼‰
"""

import requests, json, time, os, sys, csv
from datetime import datetime, timedelta
from collections import defaultdict

TUSHARE_TOKEN = '8a2c71af4fbc6faf83da2ad4404c1c47f41983562cc9fb2fa6dd4fae'
TUSHARE_URL = 'https://api.tushare.pro'
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
OUTPUT_JSON = os.path.join(BASE_DIR, 'momentum_sentiment.json')
RAW_CSV = os.path.join(BASE_DIR, 'momentum_raw.csv')
FULL_CSV = os.path.join(BASE_DIR, 'momentum_sentiment.csv')
CACHE_DIR = os.path.join(BASE_DIR, '_cache')
LOOKBACK_DAYS = 120

os.makedirs(CACHE_DIR, exist_ok=True)

def log(msg):
    print(msg, flush=True)


# â•â•â• Tushare API â•â•â•

def tushare_call(api_name, params, retries=3):
    for attempt in range(retries):
        try:
            resp = requests.post(TUSHARE_URL, json={
                'api_name': api_name, 'token': TUSHARE_TOKEN,
                'params': params, 'fields': ''
            }, timeout=20)
            data = resp.json()
            if data.get('code') == 0 and data.get('data'):
                cols = data['data']['fields']
                return [dict(zip(cols, row)) for row in data['data']['items']]
            return []
        except Exception as e:
            if attempt < retries - 1:
                time.sleep(2)
    return []


def get_trade_dates(n_days=LOOKBACK_DAYS):
    end = datetime.now().strftime('%Y%m%d')
    start = (datetime.now() - timedelta(days=n_days * 2)).strftime('%Y%m%d')
    data = tushare_call('trade_cal', {
        'exchange': 'SSE', 'start_date': start,
        'end_date': end, 'is_open': '1'
    })
    if not data:
        return []
    return sorted([d['cal_date'] for d in data])[-n_days:]


# â•â•â• CSV å·¥å…· â•â•â•

RAW_HEADERS = [
    'date', 'up_count', 'down_count', 'zha_count', 'max_height',
    'lianban_count', 'shouban_count', 'seal_zero_count',
    'big_cap_up', 'mega_cap_up', 'mega_cap_names'
]

FULL_HEADERS = [
    # åŸºç¡€æ•°æ®
    'date', 'up_count', 'down_count', 'zha_count', 'max_height',
    'lianban_count', 'shouban_count', 'seal_zero_count',
    'big_cap_up', 'mega_cap_up', 'mega_cap_names',
    # è®¡ç®—æŒ‡æ ‡
    'promotion_rate', 'rate_1to2', 'zha_rate', 'ud_ratio', 'seal_quality',
    'h_norm', 'p_norm', 'z_norm', 'u_norm', 's_norm',
    'sentiment', 'cycle_label',
    # è®¡ç®—å…¬å¼
    'formula_promotion_rate', 'formula_rate_1to2', 'formula_zha_rate',
    'formula_ud_ratio', 'formula_seal_quality', 'formula_sentiment',
    'formula_cycle_label'
]

def read_csv_dates(path):
    """è¯»å–CSVä¸­å·²æœ‰çš„æ—¥æœŸé›†åˆ"""
    if not os.path.exists(path):
        return set()
    with open(path, 'r', newline='', encoding='gb18030') as f:
        reader = csv.DictReader(f)
        return set(row['date'] for row in reader)

def read_raw_csv():
    """è¯»å–åŸå§‹CSVï¼Œè¿”å›æŒ‰æ—¥æœŸæ’åºçš„åˆ—è¡¨"""
    if not os.path.exists(RAW_CSV):
        return []
    with open(RAW_CSV, 'r', newline='', encoding='gb18030') as f:
        reader = csv.DictReader(f)
        rows = list(reader)
    # è½¬æ•°å€¼
    for r in rows:
        for k in RAW_HEADERS[1:]:
            if k == 'mega_cap_names':
                r[k] = r.get(k, '')
            else:
                r[k] = int(r[k]) if r.get(k, '') != '' else 0
    return sorted(rows, key=lambda x: x['date'])

def write_csv(path, headers, rows):
    with open(path, 'w', newline='', encoding='gb18030') as f:
        writer = csv.DictWriter(f, fieldnames=headers)
        writer.writeheader()
        writer.writerows(rows)

def append_csv(path, headers, rows):
    exists = os.path.exists(path) and os.path.getsize(path) > 0
    with open(path, 'a', newline='', encoding='gb18030') as f:
        writer = csv.DictWriter(f, fieldnames=headers)
        if not exists:
            writer.writeheader()
        writer.writerows(rows)


# â•â•â• ä»å·²æœ‰ JSON è¿ç§»åˆ° CSV â•â•â•

def migrate_from_json():
    """é¦–æ¬¡è¿ç§»ï¼šä» momentum_sentiment.json å¯¼å…¥åˆ° raw CSV"""
    if not os.path.exists(OUTPUT_JSON):
        return False
    if os.path.exists(RAW_CSV) and os.path.getsize(RAW_CSV) > 100:
        return True  # å·²è¿ç§»è¿‡

    log("  ä» momentum_sentiment.json è¿ç§»åˆ° CSV...")
    with open(OUTPUT_JSON) as f:
        data = json.load(f)

    raw_rows = []
    for d in data.get('daily', []):
        raw_rows.append({
            'date': d['date'],
            'up_count': d['up_count'],
            'down_count': d['down_count'],
            'zha_count': d['zha_count'],
            'max_height': d['max_height'],
            'lianban_count': d['lianban_count'],
            'shouban_count': d['shouban_count'],
            'seal_zero_count': round(d['seal_quality'] / 100 * max(d['up_count'], 1)),
            'big_cap_up': 0,
            'mega_cap_up': 0,
            'mega_cap_names': '',
        })
    write_csv(RAW_CSV, RAW_HEADERS, raw_rows)
    log(f"    momentum_raw.csv: {len(raw_rows)} è¡Œ")
    return True


# â•â•â• æ‹‰å–å•æ—¥æ•°æ® â•â•â•

def fetch_day_cached(trade_date):
    """æ‹‰å–æŸæ—¥ U/D/Z æ•°æ®ï¼Œæœ‰ç¼“å­˜ç›´æ¥è¯»"""
    cache_file = os.path.join(CACHE_DIR, f'{trade_date}.json')
    if os.path.exists(cache_file):
        with open(cache_file) as f:
            return json.load(f)

    ups = tushare_call('limit_list_d', {'trade_date': trade_date, 'limit_type': 'U'})
    time.sleep(0.2)
    downs = tushare_call('limit_list_d', {'trade_date': trade_date, 'limit_type': 'D'})
    time.sleep(0.2)
    zhas = tushare_call('limit_list_d', {'trade_date': trade_date, 'limit_type': 'Z'})
    time.sleep(0.2)

    result = {'U': ups, 'D': downs, 'Z': zhas}
    with open(cache_file, 'w') as f:
        json.dump(result, f, ensure_ascii=False)
    return result


def fetch_daily_basic(trade_date, retries=3):
    """æ‹‰å–æŸæ—¥ daily_basic è·å–æ€»å¸‚å€¼ï¼Œè¿”å› {ts_code: total_mv} å­—å…¸"""
    for attempt in range(retries):
        try:
            resp = requests.post(TUSHARE_URL, json={
                'api_name': 'daily_basic', 'token': TUSHARE_TOKEN,
                'params': {'trade_date': trade_date},
                'fields': 'ts_code,total_mv'
            }, timeout=20)
            data = resp.json()
            if data.get('code') == 0 and data.get('data'):
                cols = data['data']['fields']
                return {row[cols.index('ts_code')]: row[cols.index('total_mv')]
                        for row in data['data']['items'] if row[cols.index('total_mv')]}
            return {}
        except Exception:
            if attempt < retries - 1:
                time.sleep(2)
    return {}


def compute_raw_day(trade_date):
    """æ‹‰å–å¹¶è®¡ç®—å•æ—¥åŸºç¡€æ•°æ®"""
    data = fetch_day_cached(trade_date)
    ups, downs, zhas = data['U'], data['D'], data['Z']

    seal_zero_count = 0
    max_height = 0
    lianban_count = shouban_count = 0

    for u in ups:
        lt = u.get('limit_times') or 1
        ot = u.get('open_times') or 0
        if lt > max_height:
            max_height = lt
        if lt > 1:
            lianban_count += 1
        else:
            shouban_count += 1
        if ot == 0:
            seal_zero_count += 1

    # å¸‚å€¼æ ‡æ³¨
    big_cap_up = 0
    mega_cap_up = 0
    mega_cap_names = []
    if ups:
        mv_map = fetch_daily_basic(trade_date)
        time.sleep(0.2)
        for u in ups:
            ts_code = u.get('ts_code', '')
            mv = mv_map.get(ts_code)
            if mv and mv >= 1000000:  # >=100äº¿
                big_cap_up += 1
            if mv and mv >= 3000000:  # >=300äº¿
                mega_cap_up += 1
                name = u.get('name', ts_code)
                mega_cap_names.append(name)

    return {
        'date': trade_date,
        'up_count': len(ups),
        'down_count': len(downs),
        'zha_count': len(zhas),
        'max_height': max_height,
        'lianban_count': lianban_count,
        'shouban_count': shouban_count,
        'seal_zero_count': seal_zero_count,
        'big_cap_up': big_cap_up,
        'mega_cap_up': mega_cap_up,
        'mega_cap_names': '|'.join(mega_cap_names),
    }


# â•â•â• è®¡ç®—æŒ‡æ ‡ï¼ˆéœ€è¦å‰åæ–‡ï¼‰ â•â•â•

def compute_all_metrics(raw_rows):
    """ä»åŸå§‹æ•°æ®è®¡ç®—æ‰€æœ‰æŒ‡æ ‡ï¼Œè¿”å›å®Œæ•´è¡Œåˆ—è¡¨"""
    # éœ€è¦ä» cache è¯»å–æ¯æ—¥æ¶¨åœä»£ç æ¥ç®—æ™‹çº§ç‡
    prev_up_codes = set()
    prev_up_by_height = defaultdict(set)

    full_rows = []

    for i, r in enumerate(raw_rows):
        dt = r['date']
        up_count = r['up_count']
        down_count = r['down_count']
        zha_count = r['zha_count']
        max_height = r['max_height']
        seal_zero_count = r['seal_zero_count']

        # ä» cache è¯»å–æ¶¨åœä»£ç ï¼ˆç”¨äºæ™‹çº§ç‡è®¡ç®—ï¼‰
        cache_file = os.path.join(CACHE_DIR, f'{dt}.json')
        current_up_codes = set()
        current_up_by_height = defaultdict(set)

        if os.path.exists(cache_file):
            with open(cache_file) as f:
                day_data = json.load(f)
            for u in day_data.get('U', []):
                ts_code = u.get('ts_code', '')
                lt = u.get('limit_times') or 1
                current_up_codes.add(ts_code)
                current_up_by_height[lt].add(ts_code)

        # æ™‹çº§ç‡ = ä»Šæ—¥æ¶¨åœä¸­æ˜¨æ—¥ä¹Ÿæ¶¨åœçš„ / æ˜¨æ—¥æ¶¨åœæ€»æ•°
        promotion_rate = 0
        if prev_up_codes:
            continued = current_up_codes & prev_up_codes
            promotion_rate = len(continued) / len(prev_up_codes) * 100

        # 1è¿›2ç‡ = æ˜¨æ—¥é¦–æ¿ä»Šæ—¥è¿æ¿çš„ / æ˜¨æ—¥é¦–æ¿æ€»æ•°
        rate_1to2 = 0
        if prev_up_by_height.get(1):
            prev_sb = prev_up_by_height[1]
            today_lb = {u for u in current_up_codes
                       if any(current_up_by_height[h] for h in current_up_by_height if h >= 2)
                       and u in current_up_codes}
            # æ›´å‡†ç¡®ï¼šä» cache ç›´æ¥å– limit_times >= 2 çš„
            today_lb = set()
            if os.path.exists(cache_file):
                for u in day_data.get('U', []):
                    if (u.get('limit_times') or 1) >= 2:
                        today_lb.add(u.get('ts_code', ''))
            promoted = prev_sb & today_lb
            rate_1to2 = len(promoted) / len(prev_sb) * 100 if prev_sb else 0

        # ç‚¸æ¿ç‡ = ç‚¸æ¿æ•° / (æ¶¨åœæ•° + ç‚¸æ¿æ•°)
        zha_rate = zha_count / max(up_count + zha_count, 1) * 100
        # æ¶¨è·Œåœæ¯” = æ¶¨åœæ•° / è·Œåœæ•°ï¼ˆclipåˆ°20é˜²æç«¯å€¼ï¼‰
        ud_ratio = min(up_count / max(down_count, 1), 20)
        # å°æ¿è´¨é‡ = å¤§å¸‚å€¼æ¶¨åœå æ¯”ï¼ˆ100äº¿+æ¶¨åœæ•°/æ€»æ¶¨åœæ•°ï¼‰ï¼Œåæ˜ èµ„é‡‘çº§åˆ«
        # æ—§ç‰ˆç”¨ä¸€å­—æ¿(open_times=0)å æ¯”ï¼Œä½†ä¸€å­—æ¿=ä¹°ä¸åˆ°â‰ èµ„é‡‘è´¨é‡
        big_cap = r.get('big_cap_up', 0)
        mega_cap = r.get('mega_cap_up', 0)
        seal_quality = (big_cap + 2 * mega_cap) / max(up_count, 1) * 100

        full_rows.append({
            'date': dt,
            'up_count': up_count,
            'down_count': down_count,
            'zha_count': zha_count,
            'max_height': max_height,
            'lianban_count': r['lianban_count'],
            'shouban_count': r['shouban_count'],
            'seal_zero_count': seal_zero_count,
            'big_cap_up': r.get('big_cap_up', 0),
            'mega_cap_up': r.get('mega_cap_up', 0),
            'mega_cap_names': r.get('mega_cap_names', ''),
            'promotion_rate': round(promotion_rate, 2),
            'rate_1to2': round(rate_1to2, 2),
            'zha_rate': round(zha_rate, 2),
            'ud_ratio': round(ud_ratio, 2),
            'seal_quality': round(seal_quality, 2),
            # å…¬å¼åˆ—
            'formula_promotion_rate': 'ä»Šæ—¥æ¶¨åœâˆ©æ˜¨æ—¥æ¶¨åœ / æ˜¨æ—¥æ¶¨åœæ€»æ•° Ã— 100',
            'formula_rate_1to2': 'æ˜¨æ—¥é¦–æ¿âˆ©ä»Šæ—¥è¿æ¿(limit_timesâ‰¥2) / æ˜¨æ—¥é¦–æ¿æ•° Ã— 100',
            'formula_zha_rate': 'zha_count / (up_count + zha_count) Ã— 100',
            'formula_ud_ratio': 'min(up_count / max(down_count, 1), 20)ï¼Œclipé˜²æç«¯å€¼',
            'formula_seal_quality': '(big_cap_up + 2*mega_cap_up) / up_count Ã— 100ï¼Œå¤§å¸‚å€¼æ¶¨åœåŠ æƒå æ¯”',
        })

        prev_up_codes = current_up_codes
        prev_up_by_height = current_up_by_height

    # æ ‡å‡†åŒ– + åˆæˆæƒ…ç»ªæŒ‡æ•°ï¼ˆéœ€è¦å…¨é‡æ•°æ®ï¼‰
    compute_sentiment(full_rows)

    return full_rows


def percentile_rank(values, window=120):
    """120æ—¥æ»šåŠ¨åˆ†ä½æ•°æ’åï¼ˆ0-100ï¼‰ï¼Œæ›¿ä»£60æ—¥min-maxæ ‡å‡†åŒ–ã€‚
    
    ä¼˜åŠ¿ï¼šæœ‰ç»å¯¹é”šç‚¹ï¼Œå†°ç‚¹æœŸé‡Œçš„"ç›¸å¯¹é«˜"ä¸ä¼šè¢«è¯¯åˆ¤ä¸ºåŠ é€Ÿã€‚
    å½“çª—å£å†…æ•°æ®ä¸è¶³æ—¶ï¼Œç”¨å·²æœ‰æ•°æ®è®¡ç®—ã€‚
    """
    result = []
    for i, v in enumerate(values):
        w = values[max(0, i - window + 1):i + 1]
        if len(w) <= 1:
            result.append(50.0)
            continue
        # åˆ†ä½æ•°ï¼šå°äºå½“å‰å€¼çš„å æ¯”
        below = sum(1 for x in w if x < v)
        equal = sum(1 for x in w if x == v)
        # ä¸­ä½æ•°æ³•åˆ†ä½ï¼š(below + 0.5*equal) / total
        rank = (below + 0.5 * equal) / len(w) * 100
        result.append(round(rank, 2))
    return result


def compute_sentiment(full_rows):
    """åœ¨ full_rows ä¸ŠåŸåœ°æ·»åŠ æ ‡å‡†åŒ–å› å­ã€åˆæˆæƒ…ç»ªã€å‘¨æœŸæ ‡ç­¾ã€‚
    
    v2 ä¼˜åŒ–ï¼ˆ2026-03-01ï¼‰ï¼š
    â‘  å°æ¿è´¨é‡æ”¹ä¸ºå¤§å¸‚å€¼æ¶¨åœå æ¯”ï¼ˆbig_cap+2*mega_cap / up_countï¼‰
    â‘¡ æ ‡å‡†åŒ–æ”¹ä¸º120æ—¥åˆ†ä½æ•°æ’åï¼ˆæ›¿ä»£60æ—¥min-maxï¼Œæœ‰ç»å¯¹é”šï¼‰
    â‘¢ æ¶¨è·Œåœæ¯”clipåˆ°20ï¼ˆé˜²è·Œåœ=0æ—¶æç«¯å€¼ï¼‰
    â‘£ åŠ äº¤äº’ä¿®æ­£é¡¹ï¼š
       - é«˜åº¦Ã—è´¨é‡äº¤äº’ï¼šè¿æ¿é«˜ä½†å°æ¿è´¨é‡ä½ï¼ˆå…¨å°ç¥¨ï¼‰â†’ æ‰“æŠ˜
       - èµšäºå¯¹å†²ï¼šæ™‹çº§ç‡é«˜ä½†ç‚¸æ¿ç‡ä¹Ÿé«˜ï¼ˆåˆ†æ­§æœŸï¼‰â†’ æ‰“æŠ˜
    â‘¤ æƒé‡è°ƒæ•´ï¼šç©ºé—´é«˜åº¦0.20 æ™‹çº§ç‡0.25 åç‚¸æ¿ç‡0.20 æ¶¨è·Œåœæ¯”0.10 å°æ¿è´¨é‡0.25
    """
    h = percentile_rank([r['max_height'] for r in full_rows])
    p = percentile_rank([r['promotion_rate'] for r in full_rows])
    z = percentile_rank([100 - r['zha_rate'] for r in full_rows])
    u = percentile_rank([r['ud_ratio'] for r in full_rows])
    s = percentile_rank([r['seal_quality'] for r in full_rows])

    sentiment = []
    for i in range(len(full_rows)):
        # åŸºç¡€åŠ æƒï¼šæå‡å°æ¿è´¨é‡æƒé‡ï¼ˆèµ„é‡‘çº§åˆ«ï¼‰ï¼Œé™ä½æ¶¨è·Œåœæ¯”æƒé‡ï¼ˆæ˜“æç«¯ï¼‰
        base = 0.20*h[i] + 0.25*p[i] + 0.20*z[i] + 0.10*u[i] + 0.25*s[i]
        
        # äº¤äº’ä¿®æ­£1ï¼šé«˜åº¦Ã—è´¨é‡ â€” è¿æ¿é«˜ä½†å°æ¿è´¨é‡ä½ï¼Œè¯´æ˜å…¨æ˜¯å°ç¥¨åœ¨ç©ï¼Œæ‰“æŠ˜
        # hé«˜sä½ â†’ æ‰£åˆ†ï¼›hé«˜sä¹Ÿé«˜ â†’ ä¸æ‰£
        if h[i] > 70 and s[i] < 30:
            base *= 0.85  # æ‰“85æŠ˜
        
        # äº¤äº’ä¿®æ­£2ï¼šèµšäºå¯¹å†² â€” æ™‹çº§ç‡é«˜ä½†ç‚¸æ¿ç‡ä¹Ÿé«˜=åˆ†æ­§æœŸï¼Œä¸æ˜¯çœŸçš„å¥½
        # pé«˜zä½ï¼ˆzæ˜¯åç‚¸æ¿ç‡ï¼Œä½=ç‚¸æ¿ç‡é«˜ï¼‰
        if p[i] > 60 and z[i] < 30:
            base *= 0.90  # æ‰“9æŠ˜
        
        sentiment.append(round(min(max(base, 0), 100), 2))

    # å‘¨æœŸæ ‡ç­¾
    labels = []
    for i, v in enumerate(sentiment):
        if i < 2:
            labels.append('â€”')
            continue
        prev, prev2 = sentiment[i-1], sentiment[i-2]
        d = v - prev
        d2 = prev - prev2
        if v < 20:
            labels.append('å†°ç‚¹')
        elif v < 35 and prev < 30 and d > 0:
            labels.append('å›æš–')
        elif v > 60 and d > 0:
            labels.append('åŠ é€Ÿ')
        elif v > 50 and d < 0:
            labels.append('åˆ†æ­§')
        elif v < 40 and prev > 45 and d < 0 and d2 < 0:
            labels.append('é€€æ½®')
        elif d > 3:
            labels.append('å›æš–')
        elif d < -3:
            labels.append('é€€æ½®')
        else:
            labels.append('éœ‡è¡')

    for i, r in enumerate(full_rows):
        r['h_norm'] = h[i]
        r['p_norm'] = p[i]
        r['z_norm'] = z[i]
        r['u_norm'] = u[i]
        r['s_norm'] = s[i]
        r['sentiment'] = sentiment[i]
        r['cycle_label'] = labels[i]
        r['formula_sentiment'] = 'v2: 0.20*h + 0.25*p + 0.20*z + 0.10*u + 0.25*s (120æ—¥åˆ†ä½æ•°æ’å) Ã— äº¤äº’ä¿®æ­£(é«˜åº¦Ã—è´¨é‡, èµšäºå¯¹å†²)'
        r['formula_cycle_label'] = 'å†°ç‚¹(<20)|å›æš–(<35ä¸”ä¸Šå‡)|åŠ é€Ÿ(>60ä¸”ä¸Šå‡)|åˆ†æ­§(>50ä¸”ä¸‹é™)|é€€æ½®(<40ä»>45è¿é™)|éœ‡è¡(å…¶ä»–)'


# â•â•â• è¾“å‡º JSONï¼ˆæ ¼å¼ä¸å˜ï¼‰â•â•â•

def build_json(full_rows):
    """ä»å®Œæ•´è¡Œåˆ—è¡¨ç”Ÿæˆ JSONï¼ˆæ ¼å¼ä¸åŸç‰ˆä¸€è‡´ï¼‰"""
    daily = []
    for r in full_rows:
        daily.append({
            'date': r['date'],
            'up_count': r['up_count'],
            'down_count': r['down_count'],
            'zha_count': r['zha_count'],
            'max_height': r['max_height'],
            'lianban_count': r['lianban_count'],
            'shouban_count': r['shouban_count'],
            'promotion_rate': r['promotion_rate'],
            'rate_1to2': r['rate_1to2'],
            'zha_rate': r['zha_rate'],
            'ud_ratio': r['ud_ratio'],
            'seal_quality': r['seal_quality'],
            'sentiment': r['sentiment'],
            'h_norm': r['h_norm'],
            'p_norm': r['p_norm'],
            'z_norm': r['z_norm'],
            'u_norm': r['u_norm'],
            's_norm': r['s_norm'],
            'cycle_label': r['cycle_label'],
            'big_cap_up': r.get('big_cap_up', 0),
            'mega_cap_up': r.get('mega_cap_up', 0),
            'mega_cap_names': r.get('mega_cap_names', ''),
        })

    output = {
        'meta': {
            'generated': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
            'date_range': f"{daily[0]['date']} ~ {daily[-1]['date']}",
            'count': len(daily),
            'weights': {'height': 0.25, 'promotion': 0.25, 'anti_zha': 0.20,
                       'ud_ratio': 0.15, 'seal_quality': 0.15}
        },
        'daily': daily
    }

    with open(OUTPUT_JSON, 'w', encoding='utf-8') as f:
        json.dump(output, f, ensure_ascii=False, indent=2)

    fsize = os.path.getsize(OUTPUT_JSON) / 1024
    log(f"  JSON: {OUTPUT_JSON} ({fsize:.0f} KB)")


# â•â•â• ä¸»æµç¨‹ â•â•â•

def main():
    log("=" * 50)
    log("å¼ºåŠ¿è‚¡æƒ…ç»ªæŒ‡æ ‡ï¼ˆCSVå¢é‡æ¨¡å¼ï¼‰")
    log("=" * 50)

    # 0. é¦–æ¬¡è¿ç§»
    log("\n[0] æ£€æŸ¥CSV / è¿ç§»...")
    migrate_from_json()

    # 1. è·å–äº¤æ˜“æ—¥
    log("\n[1] è·å–äº¤æ˜“æ—¥...")
    dates = get_trade_dates(LOOKBACK_DAYS)

    if not dates:
        log("  âš ï¸ Tushare è¿ä¸ä¸Šï¼Œä½¿ç”¨å·²æœ‰CSVæ•°æ®")
        raw_rows = read_raw_csv()
        if not raw_rows:
            log("  ERROR: æ— äº¤æ˜“æ—¥ä¸”æ— CSVæ•°æ®")
            sys.exit(1)
        dates = [r['date'] for r in raw_rows]
        log(f"  ä»CSVæ¢å¤: {len(dates)} å¤©: {dates[0]} ~ {dates[-1]}")
        full_rows = compute_all_metrics(raw_rows)
        write_csv(FULL_CSV, FULL_HEADERS, full_rows)
        build_json(full_rows)
        return

    log(f"  {len(dates)} ä¸ªäº¤æ˜“æ—¥: {dates[0]} ~ {dates[-1]}")

    # 2. æ‰¾å‡ºéœ€è¦å¢é‡æ‹‰å–çš„æ—¥æœŸ
    existing_dates = read_csv_dates(RAW_CSV)
    new_dates = sorted(set(dates) - existing_dates)

    if not new_dates:
        log(f"\n  æ‰€æœ‰ {len(dates)} å¤©æ•°æ®å·²åœ¨CSVä¸­ï¼Œæ— éœ€æ‹‰å–")
    else:
        log(f"\n  éœ€è¦å¢é‡æ‹‰å–: {len(new_dates)} å¤© ({new_dates[0]} ~ {new_dates[-1]})")

    # 3. å¢é‡æ‹‰å–æ–°æ—¥æœŸçš„åŸºç¡€æ•°æ®
    if new_dates:
        log("\n[2] æ‹‰å–æ–°æ—¥æœŸæ•°æ®...")
        new_raw_rows = []
        for i, dt in enumerate(new_dates):
            cached = os.path.exists(os.path.join(CACHE_DIR, f'{dt}.json'))
            tag = 'ğŸ“¦' if cached else 'ğŸŒ'
            log(f"  [{i+1}/{len(new_dates)}] {dt} {tag}")
            row = compute_raw_day(dt)
            new_raw_rows.append(row)
            log(f"    U={row['up_count']} D={row['down_count']} Z={row['zha_count']} H={row['max_height']} BigCap={row['big_cap_up']} MegaCap={row['mega_cap_up']}")

        # è¿½åŠ åˆ° raw CSV
        append_csv(RAW_CSV, RAW_HEADERS, new_raw_rows)
        log(f"  æ–°å¢ {len(new_raw_rows)} è¡Œåˆ° momentum_raw.csv")
    else:
        log("\n[2] è·³è¿‡æ‹‰å–ï¼ˆæ•°æ®å·²å®Œæ•´ï¼‰")

    # 4. é‡æ–°è¯»å–å…¨é‡ raw æ•°æ®ï¼Œè®¡ç®—æ‰€æœ‰æŒ‡æ ‡
    log("\n[3] è®¡ç®—æŒ‡æ ‡...")
    raw_rows = read_raw_csv()
    # åªä¿ç•™ dates èŒƒå›´å†…çš„
    date_set = set(dates)
    raw_rows = [r for r in raw_rows if r['date'] in date_set]
    raw_rows.sort(key=lambda x: x['date'])

    full_rows = compute_all_metrics(raw_rows)

    # 5. å†™å®Œæ•´ CSVï¼ˆå«é‡å†™ raw CSV ä»¥ç¡®ä¿æ–°å­—æ®µåˆ—å¤´ä¸€è‡´ï¼‰
    write_csv(RAW_CSV, RAW_HEADERS, raw_rows)
    write_csv(FULL_CSV, FULL_HEADERS, full_rows)
    log(f"  momentum_sentiment.csv: {len(full_rows)} è¡Œ")

    # 6. è¾“å‡º JSON
    log("\n[4] è¾“å‡º JSON...")
    build_json(full_rows)

    latest = full_rows[-1]
    log(f"\nâœ… å®Œæˆ: {len(full_rows)} å¤©")
    log(f"   æœ€æ–°: {latest['date']} æƒ…ç»ª={latest['sentiment']} å‘¨æœŸ={latest['cycle_label']}")
    log(f"   CSV: momentum_raw.csv + momentum_sentiment.csv")


if __name__ == '__main__':
    main()
