#!/usr/bin/env python3
"""
æ¨¡å—äºŒï¼šå•†å“å¥—åˆ©ï¼ˆè·¨å“ç§æ¯”ä»· + è·¨æœŸä»·å·®ï¼‰
ä» Tushare æ‹‰å–ä¸»è¦å•†å“æœŸè´§è¿ç»­åˆçº¦æ•°æ®ï¼Œè®¡ç®—ï¼š

A. è·¨å“ç§æ¯”ä»·ï¼ˆ4ç»„ç»å…¸å¥—åˆ©å¯¹ï¼‰ï¼š
   - èºçº¹/é“çŸ¿æ¯”å€¼ï¼ˆé’¢å‚åˆ©æ¶¦ä»£ç†ï¼‰
   - è±†æ²¹/æ£•æ¦ˆæ²¹æ¯”å€¼ï¼ˆæ²¹è„‚æ›¿ä»£ï¼‰
   - é“œ/é“æ¯”å€¼ï¼ˆæœ‰è‰²å¼ºå¼±ï¼‰
   - åŸæ²¹/ç‡ƒæ²¹æ¯”å€¼ï¼ˆè£‚è§£ä»·å·®ä»£ç†ï¼‰

B. è·¨æœŸä»·å·®ï¼ˆè¿‘æœˆ-è¿œæœˆï¼‰ï¼š
   - ä» commodity_cta/fut_daily.csv å¤ç”¨è¿ç»­åˆçº¦æ•°æ®
   - ç”¨è¿ç»­åˆçº¦ close ä¸ pre_close çš„å…³ç³»æ¨ç®— contango/backwardation çŠ¶æ€

è¾“å‡ºï¼šmod2_commodity_arb.json + mod2_commodity_arb.csvï¼ˆè¿‘30ä¸ªäº¤æ˜“æ—¥ï¼‰
"""

import requests, json, time, os, sys, csv, math
from datetime import datetime, timedelta
from collections import defaultdict

# ============ é…ç½® ============
TS_URL = 'https://api.tushare.pro'
TS_TOKEN = '8a2c71af4fbc6faf83da2ad4404c1c47f41983562cc9fb2fa6dd4fae'

BASE_DIR = os.path.dirname(os.path.abspath(__file__))
OUT_JSON = os.path.join(BASE_DIR, 'mod2_commodity_arb.json')
OUT_CSV = os.path.join(BASE_DIR, 'mod2_commodity_arb.csv')

# commodity_cta çš„æœŸè´§æ•°æ®
CTA_FUT_CSV = os.path.join(BASE_DIR, '..', 'commodity_cta', 'fut_daily.csv')

LOOKBACK_DAYS = 30

# è·¨å“ç§å¥—åˆ©å¯¹ï¼š(åç§°, åˆ†å­å“ç§, åˆ†æ¯å“ç§, è¯´æ˜)
SPREAD_PAIRS = [
    ('èºçº¹/é“çŸ¿', 'RB', 'I', 'é’¢å‚åˆ©æ¶¦ä»£ç†ï¼šèºçº¹é’¢ä»·æ ¼Ã·é“çŸ¿çŸ³ä»·æ ¼'),
    ('è±†æ²¹/æ£•æ¦ˆæ²¹', 'Y', 'P', 'æ²¹è„‚æ›¿ä»£ï¼šè±†æ²¹Ã·æ£•æ¦ˆæ²¹ï¼Œé«˜ä½æ—¶æ£•æ¦ˆæ²¹ç›¸å¯¹ä¾¿å®œ'),
    ('é“œ/é“', 'CU', 'AL', 'æœ‰è‰²å¼ºå¼±ï¼šé“œÃ·é“ï¼Œåæ˜ ç»æµé¢„æœŸåˆ†åŒ–'),
    ('åŸæ²¹/ç‡ƒæ²¹', 'SC', 'FU', 'è£‚è§£ä»·å·®ä»£ç†ï¼šåŸæ²¹Ã·ç‡ƒæ²¹ï¼Œåæ˜ ç‚¼åŒ–åˆ©æ¶¦'),
]


def log(msg):
    print(msg, flush=True)


# ============ Tushare API ============
_last_call = 0

def ts_api(api_name, params, fields=None):
    global _last_call
    elapsed = time.time() - _last_call
    if elapsed < 1.5:
        time.sleep(1.5 - elapsed)

    body = {'api_name': api_name, 'token': TS_TOKEN, 'params': params}
    if fields:
        body['fields'] = fields

    for attempt in range(3):
        try:
            _last_call = time.time()
            r = requests.post(TS_URL, json=body, timeout=60)
            if not r.text:
                time.sleep(3)
                continue
            data = r.json()
            if data.get('code') == 0 and data.get('data', {}).get('items'):
                cols = data['data']['fields']
                rows = data['data']['items']
                return [dict(zip(cols, row)) for row in rows]
            elif data.get('code') == -2001:
                log(f'    [é™æµ, ç­‰10s...]')
                time.sleep(10)
                continue
            else:
                return []
        except Exception as e:
            log(f'    [å¼‚å¸¸: {e}, retry {attempt+1}]')
            time.sleep(3)
    return []


def get_trade_dates(n_days):
    end = datetime.now().strftime('%Y%m%d')
    start = (datetime.now() - timedelta(days=n_days * 3)).strftime('%Y%m%d')
    rows = ts_api('trade_cal', {
        'exchange': 'SSE', 'start_date': start,
        'end_date': end, 'is_open': '1'
    }, fields='cal_date')
    if not rows:
        return []
    return sorted([r['cal_date'] for r in rows])[-n_days:]


# ============ æ•°æ®åŠ è½½ ============

def load_cta_fut_csv():
    """ä» commodity_cta/fut_daily.csv åŠ è½½æœŸè´§æ•°æ®"""
    path = os.path.realpath(CTA_FUT_CSV)
    if not os.path.exists(path):
        log(f'  âš ï¸ {path} ä¸å­˜åœ¨ï¼Œå°†ä» API æ‹‰å–')
        return {}

    series = defaultdict(dict)  # {symbol: {date: {close, vol, amount, oi}}}
    with open(path, 'r', newline='', encoding='gb18030') as f:
        for row in csv.DictReader(f):
            sym = row.get('symbol', '')
            dt = row.get('trade_date', '')
            close = row.get('close', '')
            if not sym or not dt or not close:
                continue
            series[sym][dt] = {
                'close': float(close),
                'vol': float(row.get('vol', 0) or 0),
                'amount': float(row.get('amount', 0) or 0),
                'oi': float(row.get('oi', 0) or 0),
            }
    log(f'  ä» commodity_cta/fut_daily.csv åŠ è½½: {len(series)} å“ç§')
    return dict(series)


def fetch_fut_from_api(symbol, start_date, end_date):
    """ä» API æ‹‰å–å•å“ç§è¿ç»­åˆçº¦"""
    # ç¡®å®šäº¤æ˜“æ‰€åç¼€
    exchange_map = {
        'RB': 'SHF', 'HC': 'SHF', 'I': 'DCE', 'J': 'DCE', 'JM': 'DCE',
        'CU': 'SHF', 'AL': 'SHF', 'ZN': 'SHF', 'NI': 'SHF', 'SN': 'SHF',
        'AU': 'SHF', 'AG': 'SHF',
        'SC': 'INE', 'FU': 'SHF', 'LU': 'INE', 'BU': 'SHF',
        'MA': 'ZCE', 'TA': 'ZCE', 'SA': 'ZCE', 'FG': 'ZCE',
        'PP': 'DCE', 'L': 'DCE', 'V': 'DCE', 'EB': 'DCE', 'EG': 'DCE',
        'Y': 'DCE', 'P': 'DCE', 'M': 'DCE', 'A': 'DCE',
        'CF': 'ZCE', 'SR': 'ZCE', 'OI': 'ZCE', 'RM': 'ZCE',
        'C': 'DCE', 'CS': 'DCE', 'JD': 'DCE', 'LH': 'DCE',
        'RU': 'SHF', 'NR': 'INE', 'SP': 'SHF', 'SS': 'SHF',
        'PG': 'DCE', 'PF': 'DCE', 'PX': 'ZCE',
    }
    exch = exchange_map.get(symbol, 'SHF')
    ts_code = f'{symbol}.{exch}'

    rows = ts_api('fut_daily', {
        'ts_code': ts_code,
        'start_date': start_date,
        'end_date': end_date,
    }, fields='ts_code,trade_date,close,vol,amount,oi')

    result = {}
    for r in (rows or []):
        result[r['trade_date']] = {
            'close': r.get('close'),
            'vol': r.get('vol', 0),
            'amount': r.get('amount', 0),
            'oi': r.get('oi', 0),
        }
    return result


# ============ è®¡ç®— ============

def compute_spread_ratio(dates, series_a, series_b):
    """è®¡ç®—æ¯”ä»·åºåˆ—ï¼šA/B"""
    results = []
    ratios = []
    for d in dates:
        a = series_a.get(d)
        b = series_b.get(d)
        if not a or not b:
            continue
        ca = a.get('close')
        cb = b.get('close')
        if not ca or not cb or cb == 0:
            continue
        ratio = ca / cb
        ratios.append(ratio)
        results.append({
            'date': d,
            'close_a': round(ca, 2),
            'close_b': round(cb, 2),
            'ratio': round(ratio, 4),
        })

    # åŠ ä¸Šåˆ†ä½æ•°ï¼ˆåŸºäºå½“å‰çª—å£ï¼‰
    if ratios:
        sorted_r = sorted(ratios)
        for row in results:
            r = row['ratio']
            pctile = sum(1 for x in sorted_r if x <= r) / len(sorted_r)
            row['pctile'] = round(pctile, 4)

        # 20æ—¥å‡å€¼å’Œæ ‡å‡†å·®
        recent = ratios[-20:] if len(ratios) >= 20 else ratios
        mean = sum(recent) / len(recent)
        std = math.sqrt(sum((x - mean)**2 for x in recent) / len(recent)) if len(recent) > 1 else 0
        for row in results:
            row['z_score'] = round((row['ratio'] - mean) / std, 4) if std > 0 else 0

    return results


# ============ è¾“å‡º ============

def write_output(spread_data, dates):
    """è¾“å‡º JSON + CSV"""

    # === JSON ===
    json_out = {
        'update_time': datetime.now().strftime('%Y-%m-%d %H:%M'),
        'date_range': f'{dates[0]} ~ {dates[-1]}' if dates else '',
        'n_days': len(dates),
        'spreads': {},
        'summary': {},
    }

    for name, sym_a, sym_b, desc in SPREAD_PAIRS:
        key = f'{sym_a}_{sym_b}'
        series = spread_data.get(key, [])
        json_out['spreads'][key] = {
            'name': name,
            'symbol_a': sym_a,
            'symbol_b': sym_b,
            'description': desc,
            'series': series,
        }

        if series:
            latest = series[-1]
            all_ratios = [s['ratio'] for s in series]
            json_out['summary'][key] = {
                'name': name,
                'latest_date': latest['date'],
                'latest_ratio': latest['ratio'],
                'latest_z_score': latest.get('z_score', 0),
                'latest_pctile': latest.get('pctile', 0),
                'avg_ratio': round(sum(all_ratios) / len(all_ratios), 4),
                'max_ratio': round(max(all_ratios), 4),
                'min_ratio': round(min(all_ratios), 4),
                'close_a': latest['close_a'],
                'close_b': latest['close_b'],
            }

    with open(OUT_JSON, 'w', encoding='utf-8') as f:
        json.dump(json_out, f, ensure_ascii=False, indent=2)

    # === CSV ===
    csv_headers = [
        'date', 'pair', 'name', 'symbol_a', 'symbol_b',
        'close_a', 'close_b', 'ratio', 'pctile', 'z_score',
    ]
    csv_rows = []
    for name, sym_a, sym_b, desc in SPREAD_PAIRS:
        key = f'{sym_a}_{sym_b}'
        for row in spread_data.get(key, []):
            csv_rows.append({
                'date': row['date'],
                'pair': key,
                'name': name,
                'symbol_a': sym_a,
                'symbol_b': sym_b,
                'close_a': row['close_a'],
                'close_b': row['close_b'],
                'ratio': row['ratio'],
                'pctile': row.get('pctile', ''),
                'z_score': row.get('z_score', ''),
            })

    csv_rows.sort(key=lambda x: (x['date'], x['pair']))

    with open(OUT_CSV, 'w', newline='', encoding='utf-8') as f:
        writer = csv.DictWriter(f, fieldnames=csv_headers)
        writer.writeheader()
        writer.writerows(csv_rows)


# ============ ä¸»æµç¨‹ ============

def main():
    log('=' * 50)
    log('æ¨¡å—äºŒï¼šå•†å“å¥—åˆ©ï¼ˆè·¨å“ç§æ¯”ä»·ï¼‰')
    log('=' * 50)

    # 1. äº¤æ˜“æ—¥
    log('\n[1] è·å–äº¤æ˜“æ—¥...')
    dates = get_trade_dates(LOOKBACK_DAYS)
    if not dates:
        log('  âš ï¸ æ— æ³•è·å–äº¤æ˜“æ—¥')
        return
    start_date = dates[0]
    end_date = dates[-1]
    log(f'  {len(dates)} ä¸ªäº¤æ˜“æ—¥: {start_date} ~ {end_date}')

    # 2. åŠ è½½å·²æœ‰æ•°æ®
    log('\n[2] åŠ è½½æ•°æ®...')
    cta_data = load_cta_fut_csv()

    # æ”¶é›†éœ€è¦çš„å“ç§
    needed_symbols = set()
    for _, sym_a, sym_b, _ in SPREAD_PAIRS:
        needed_symbols.add(sym_a)
        needed_symbols.add(sym_b)

    # æ£€æŸ¥å“ªäº›å“ç§åœ¨ cta_data ä¸­æœ‰è¶³å¤Ÿæ•°æ®
    symbol_data = {}
    for sym in needed_symbols:
        if sym in cta_data:
            # æ£€æŸ¥æ—¥æœŸè¦†ç›–
            available = sum(1 for d in dates if d in cta_data[sym])
            if available >= len(dates) * 0.8:
                symbol_data[sym] = cta_data[sym]
                log(f'  {sym}: ä» CSV å¤ç”¨ ({available}/{len(dates)} å¤©)')
                continue

        # ä» API æ‹‰
        log(f'  {sym}: ä» API æ‹‰å–...')
        symbol_data[sym] = fetch_fut_from_api(sym, start_date, end_date)
        log(f'    {len(symbol_data[sym])} å¤©')

    # 3. è®¡ç®—æ¯”ä»·
    log('\n[3] è®¡ç®—è·¨å“ç§æ¯”ä»·...')
    spread_data = {}
    for name, sym_a, sym_b, desc in SPREAD_PAIRS:
        key = f'{sym_a}_{sym_b}'
        series_a = symbol_data.get(sym_a, {})
        series_b = symbol_data.get(sym_b, {})
        series = compute_spread_ratio(dates, series_a, series_b)
        spread_data[key] = series
        log(f'  {name}: {len(series)} å¤©')

    # 4. è¾“å‡º
    log('\n[4] è¾“å‡º...')
    write_output(spread_data, dates)

    log(f'\nâœ… æ¨¡å—äºŒå®Œæˆ')
    log(f'  JSON: {OUT_JSON}')
    log(f'  CSV:  {OUT_CSV}')

    # æ‰“å°æ±‡æ€»
    log(f'\n{"â”€"*55}')
    log(f'ğŸ“Š å•†å“å¥—åˆ©æ¯”ä»·æ±‡æ€» ({end_date})')
    log(f'{"â”€"*55}')
    for name, sym_a, sym_b, desc in SPREAD_PAIRS:
        key = f'{sym_a}_{sym_b}'
        series = spread_data.get(key, [])
        if series:
            latest = series[-1]
            all_ratios = [s['ratio'] for s in series]
            avg = sum(all_ratios) / len(all_ratios)
            log(f'  {name:>8s}: '
                f'æ¯”å€¼={latest["ratio"]:.4f}  '
                f'Z={latest.get("z_score", 0):>+6.2f}  '
                f'åˆ†ä½={latest.get("pctile", 0):.0%}  '
                f'30æ—¥å‡å€¼={avg:.4f}')


if __name__ == '__main__':
    main()
