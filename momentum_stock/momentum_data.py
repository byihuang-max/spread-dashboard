#!/usr/bin/env python3
"""
å¼ºåŠ¿è‚¡æƒ…ç»ªæŒ‡æ ‡æ•°æ®è„šæœ¬ï¼ˆCSVå¢é‡æ¨¡å¼ï¼‰
ä» Tushare limit_list_d æ‹‰å–æ¶¨è·Œåœæ•°æ®ï¼Œå¢é‡è¿½åŠ åˆ° CSV
CSV åŒ…å«ï¼šåŸºç¡€æ•°æ®åˆ— + è®¡ç®—æŒ‡æ ‡åˆ— + è®¡ç®—å…¬å¼åˆ—

å¢é‡ç­–ç•¥ï¼š
- momentum_raw.csv å­˜åŸºç¡€æ•°æ®ï¼ˆæ¯æ—¥æ¶¨è·ŒåœåŸå§‹ç»Ÿè®¡ï¼‰
- momentum_sentiment.csv å­˜å®Œæ•´æ•°æ®ï¼ˆåŸºç¡€+è®¡ç®—æŒ‡æ ‡+å…¬å¼ï¼‰
- æ¯æ¬¡åªæ‹‰ CSV ä¸­æ²¡æœ‰çš„æ–°æ—¥æœŸ
- æœ€åä»è¾“å‡º momentum_sentiment.jsonï¼ˆä¾› inject_momentum.py ä½¿ç”¨ï¼‰
"""

import requests, json, time, os, sys, csv
from datetime import datetime, timedelta
from collections import defaultdict

TUSHARE_TOKEN = '33b3ff939d0d7954cd76cacce7cf6cbb2b3c3feda13d1ca2cfa594e20ecd'
TUSHARE_URL = 'http://lianghua.nanyangqiankun.top'
BASE_DIR = '/Users/apple/Desktop/gamt-dashboard/momentum_stock'
OUTPUT_JSON = os.path.join(BASE_DIR, 'momentum_sentiment.json')
RAW_CSV = os.path.join(BASE_DIR, 'momentum_raw.csv')
FULL_CSV = os.path.join(BASE_DIR, 'momentum_sentiment.csv')
CACHE_DIR = os.path.join(BASE_DIR, '_cache')
LOOKBACK_DAYS = 120

os.makedirs(CACHE_DIR, exist_ok=True)

def log(msg):
    print(msg, flush=True)


# â•â•â• Tushare API â•â•â•

def tushare_call(api_name, params, retries=3):
    for attempt in range(retries):
        try:
            resp = requests.post(TUSHARE_URL, json={
                'api_name': api_name, 'token': TUSHARE_TOKEN,
                'params': params, 'fields': ''
            }, timeout=20)
            data = resp.json()
            if data.get('code') == 0 and data.get('data'):
                cols = data['data']['fields']
                return [dict(zip(cols, row)) for row in data['data']['items']]
            return []
        except Exception as e:
            if attempt < retries - 1:
                time.sleep(2)
    return []


def get_trade_dates(n_days=LOOKBACK_DAYS):
    end = datetime.now().strftime('%Y%m%d')
    start = (datetime.now() - timedelta(days=n_days * 2)).strftime('%Y%m%d')
    data = tushare_call('trade_cal', {
        'exchange': 'SSE', 'start_date': start,
        'end_date': end, 'is_open': '1'
    })
    if not data:
        return []
    return sorted([d['cal_date'] for d in data])[-n_days:]


# â•â•â• CSV å·¥å…· â•â•â•

RAW_HEADERS = [
    'date', 'up_count', 'down_count', 'zha_count', 'max_height',
    'lianban_count', 'shouban_count', 'seal_zero_count'
]

FULL_HEADERS = [
    # åŸºç¡€æ•°æ®
    'date', 'up_count', 'down_count', 'zha_count', 'max_height',
    'lianban_count', 'shouban_count', 'seal_zero_count',
    # è®¡ç®—æŒ‡æ ‡
    'promotion_rate', 'rate_1to2', 'zha_rate', 'ud_ratio', 'seal_quality',
    'h_norm', 'p_norm', 'z_norm', 'u_norm', 's_norm',
    'sentiment', 'cycle_label',
    # è®¡ç®—å…¬å¼
    'formula_promotion_rate', 'formula_rate_1to2', 'formula_zha_rate',
    'formula_ud_ratio', 'formula_seal_quality', 'formula_sentiment',
    'formula_cycle_label'
]

def read_csv_dates(path):
    """è¯»å–CSVä¸­å·²æœ‰çš„æ—¥æœŸé›†åˆ"""
    if not os.path.exists(path):
        return set()
    with open(path, 'r', newline='', encoding='gb18030') as f:
        reader = csv.DictReader(f)
        return set(row['date'] for row in reader)

def read_raw_csv():
    """è¯»å–åŸå§‹CSVï¼Œè¿”å›æŒ‰æ—¥æœŸæ’åºçš„åˆ—è¡¨"""
    if not os.path.exists(RAW_CSV):
        return []
    with open(RAW_CSV, 'r', newline='', encoding='gb18030') as f:
        reader = csv.DictReader(f)
        rows = list(reader)
    # è½¬æ•°å€¼
    for r in rows:
        for k in RAW_HEADERS[1:]:
            r[k] = int(r[k]) if r.get(k, '') != '' else 0
    return sorted(rows, key=lambda x: x['date'])

def write_csv(path, headers, rows):
    with open(path, 'w', newline='', encoding='gb18030') as f:
        writer = csv.DictWriter(f, fieldnames=headers)
        writer.writeheader()
        writer.writerows(rows)

def append_csv(path, headers, rows):
    exists = os.path.exists(path) and os.path.getsize(path) > 0
    with open(path, 'a', newline='', encoding='gb18030') as f:
        writer = csv.DictWriter(f, fieldnames=headers)
        if not exists:
            writer.writeheader()
        writer.writerows(rows)


# â•â•â• ä»å·²æœ‰ JSON è¿ç§»åˆ° CSV â•â•â•

def migrate_from_json():
    """é¦–æ¬¡è¿ç§»ï¼šä» momentum_sentiment.json å¯¼å…¥åˆ° raw CSV"""
    if not os.path.exists(OUTPUT_JSON):
        return False
    if os.path.exists(RAW_CSV) and os.path.getsize(RAW_CSV) > 100:
        return True  # å·²è¿ç§»è¿‡

    log("  ä» momentum_sentiment.json è¿ç§»åˆ° CSV...")
    with open(OUTPUT_JSON) as f:
        data = json.load(f)

    raw_rows = []
    for d in data.get('daily', []):
        raw_rows.append({
            'date': d['date'],
            'up_count': d['up_count'],
            'down_count': d['down_count'],
            'zha_count': d['zha_count'],
            'max_height': d['max_height'],
            'lianban_count': d['lianban_count'],
            'shouban_count': d['shouban_count'],
            'seal_zero_count': round(d['seal_quality'] / 100 * max(d['up_count'], 1)),
        })
    write_csv(RAW_CSV, RAW_HEADERS, raw_rows)
    log(f"    momentum_raw.csv: {len(raw_rows)} è¡Œ")
    return True


# â•â•â• æ‹‰å–å•æ—¥æ•°æ® â•â•â•

def fetch_day_cached(trade_date):
    """æ‹‰å–æŸæ—¥ U/D/Z æ•°æ®ï¼Œæœ‰ç¼“å­˜ç›´æ¥è¯»"""
    cache_file = os.path.join(CACHE_DIR, f'{trade_date}.json')
    if os.path.exists(cache_file):
        with open(cache_file) as f:
            return json.load(f)

    ups = tushare_call('limit_list_d', {'trade_date': trade_date, 'limit_type': 'U'})
    time.sleep(0.2)
    downs = tushare_call('limit_list_d', {'trade_date': trade_date, 'limit_type': 'D'})
    time.sleep(0.2)
    zhas = tushare_call('limit_list_d', {'trade_date': trade_date, 'limit_type': 'Z'})
    time.sleep(0.2)

    result = {'U': ups, 'D': downs, 'Z': zhas}
    with open(cache_file, 'w') as f:
        json.dump(result, f, ensure_ascii=False)
    return result


def compute_raw_day(trade_date):
    """æ‹‰å–å¹¶è®¡ç®—å•æ—¥åŸºç¡€æ•°æ®"""
    data = fetch_day_cached(trade_date)
    ups, downs, zhas = data['U'], data['D'], data['Z']

    seal_zero_count = 0
    max_height = 0
    lianban_count = shouban_count = 0

    for u in ups:
        lt = u.get('limit_times') or 1
        ot = u.get('open_times') or 0
        if lt > max_height:
            max_height = lt
        if lt > 1:
            lianban_count += 1
        else:
            shouban_count += 1
        if ot == 0:
            seal_zero_count += 1

    return {
        'date': trade_date,
        'up_count': len(ups),
        'down_count': len(downs),
        'zha_count': len(zhas),
        'max_height': max_height,
        'lianban_count': lianban_count,
        'shouban_count': shouban_count,
        'seal_zero_count': seal_zero_count,
    }


# â•â•â• è®¡ç®—æŒ‡æ ‡ï¼ˆéœ€è¦å‰åæ–‡ï¼‰ â•â•â•

def compute_all_metrics(raw_rows):
    """ä»åŸå§‹æ•°æ®è®¡ç®—æ‰€æœ‰æŒ‡æ ‡ï¼Œè¿”å›å®Œæ•´è¡Œåˆ—è¡¨"""
    # éœ€è¦ä» cache è¯»å–æ¯æ—¥æ¶¨åœä»£ç æ¥ç®—æ™‹çº§ç‡
    prev_up_codes = set()
    prev_up_by_height = defaultdict(set)

    full_rows = []

    for i, r in enumerate(raw_rows):
        dt = r['date']
        up_count = r['up_count']
        down_count = r['down_count']
        zha_count = r['zha_count']
        max_height = r['max_height']
        seal_zero_count = r['seal_zero_count']

        # ä» cache è¯»å–æ¶¨åœä»£ç ï¼ˆç”¨äºæ™‹çº§ç‡è®¡ç®—ï¼‰
        cache_file = os.path.join(CACHE_DIR, f'{dt}.json')
        current_up_codes = set()
        current_up_by_height = defaultdict(set)

        if os.path.exists(cache_file):
            with open(cache_file) as f:
                day_data = json.load(f)
            for u in day_data.get('U', []):
                ts_code = u.get('ts_code', '')
                lt = u.get('limit_times') or 1
                current_up_codes.add(ts_code)
                current_up_by_height[lt].add(ts_code)

        # æ™‹çº§ç‡ = ä»Šæ—¥æ¶¨åœä¸­æ˜¨æ—¥ä¹Ÿæ¶¨åœçš„ / æ˜¨æ—¥æ¶¨åœæ€»æ•°
        promotion_rate = 0
        if prev_up_codes:
            continued = current_up_codes & prev_up_codes
            promotion_rate = len(continued) / len(prev_up_codes) * 100

        # 1è¿›2ç‡ = æ˜¨æ—¥é¦–æ¿ä»Šæ—¥è¿æ¿çš„ / æ˜¨æ—¥é¦–æ¿æ€»æ•°
        rate_1to2 = 0
        if prev_up_by_height.get(1):
            prev_sb = prev_up_by_height[1]
            today_lb = {u for u in current_up_codes
                       if any(current_up_by_height[h] for h in current_up_by_height if h >= 2)
                       and u in current_up_codes}
            # æ›´å‡†ç¡®ï¼šä» cache ç›´æ¥å– limit_times >= 2 çš„
            today_lb = set()
            if os.path.exists(cache_file):
                for u in day_data.get('U', []):
                    if (u.get('limit_times') or 1) >= 2:
                        today_lb.add(u.get('ts_code', ''))
            promoted = prev_sb & today_lb
            rate_1to2 = len(promoted) / len(prev_sb) * 100 if prev_sb else 0

        # ç‚¸æ¿ç‡ = ç‚¸æ¿æ•° / (æ¶¨åœæ•° + ç‚¸æ¿æ•°)
        zha_rate = zha_count / max(up_count + zha_count, 1) * 100
        # æ¶¨è·Œåœæ¯” = æ¶¨åœæ•° / è·Œåœæ•°
        ud_ratio = up_count / max(down_count, 1)
        # å°æ¿è´¨é‡ = ä¸€å­—/ç§’æ¿(open_times=0)å æ¯”
        seal_quality = seal_zero_count / max(up_count, 1) * 100

        full_rows.append({
            'date': dt,
            'up_count': up_count,
            'down_count': down_count,
            'zha_count': zha_count,
            'max_height': max_height,
            'lianban_count': r['lianban_count'],
            'shouban_count': r['shouban_count'],
            'seal_zero_count': seal_zero_count,
            'promotion_rate': round(promotion_rate, 2),
            'rate_1to2': round(rate_1to2, 2),
            'zha_rate': round(zha_rate, 2),
            'ud_ratio': round(ud_ratio, 2),
            'seal_quality': round(seal_quality, 2),
            # å…¬å¼åˆ—
            'formula_promotion_rate': 'ä»Šæ—¥æ¶¨åœâˆ©æ˜¨æ—¥æ¶¨åœ / æ˜¨æ—¥æ¶¨åœæ€»æ•° Ã— 100',
            'formula_rate_1to2': 'æ˜¨æ—¥é¦–æ¿âˆ©ä»Šæ—¥è¿æ¿(limit_timesâ‰¥2) / æ˜¨æ—¥é¦–æ¿æ•° Ã— 100',
            'formula_zha_rate': 'zha_count / (up_count + zha_count) Ã— 100',
            'formula_ud_ratio': 'up_count / max(down_count, 1)',
            'formula_seal_quality': 'seal_zero_count(open_times=0) / up_count Ã— 100',
        })

        prev_up_codes = current_up_codes
        prev_up_by_height = current_up_by_height

    # æ ‡å‡†åŒ– + åˆæˆæƒ…ç»ªæŒ‡æ•°ï¼ˆéœ€è¦å…¨é‡æ•°æ®ï¼‰
    compute_sentiment(full_rows)

    return full_rows


def normalize_series(values, window=60):
    result = []
    for i, v in enumerate(values):
        w = values[max(0, i - window + 1):i + 1]
        mn, mx = min(w), max(w)
        result.append(round((v - mn) / (mx - mn) * 100, 2) if mx != mn else 50.0)
    return result


def compute_sentiment(full_rows):
    """åœ¨ full_rows ä¸ŠåŸåœ°æ·»åŠ æ ‡å‡†åŒ–å› å­ã€åˆæˆæƒ…ç»ªã€å‘¨æœŸæ ‡ç­¾"""
    h = normalize_series([r['max_height'] for r in full_rows])
    p = normalize_series([r['promotion_rate'] for r in full_rows])
    z = normalize_series([100 - r['zha_rate'] for r in full_rows])
    u = normalize_series([r['ud_ratio'] for r in full_rows])
    s = normalize_series([r['seal_quality'] for r in full_rows])

    sentiment = [round(0.25*h[i] + 0.25*p[i] + 0.20*z[i] + 0.15*u[i] + 0.15*s[i], 2)
                 for i in range(len(full_rows))]

    # å‘¨æœŸæ ‡ç­¾
    labels = []
    for i, v in enumerate(sentiment):
        if i < 2:
            labels.append('â€”')
            continue
        prev, prev2 = sentiment[i-1], sentiment[i-2]
        d = v - prev
        d2 = prev - prev2
        if v < 20:
            labels.append('å†°ç‚¹')
        elif v < 35 and prev < 30 and d > 0:
            labels.append('å›æš–')
        elif v > 60 and d > 0:
            labels.append('åŠ é€Ÿ')
        elif v > 50 and d < 0:
            labels.append('åˆ†æ­§')
        elif v < 40 and prev > 45 and d < 0 and d2 < 0:
            labels.append('é€€æ½®')
        elif d > 3:
            labels.append('å›æš–')
        elif d < -3:
            labels.append('é€€æ½®')
        else:
            labels.append('éœ‡è¡')

    for i, r in enumerate(full_rows):
        r['h_norm'] = h[i]
        r['p_norm'] = p[i]
        r['z_norm'] = z[i]
        r['u_norm'] = u[i]
        r['s_norm'] = s[i]
        r['sentiment'] = sentiment[i]
        r['cycle_label'] = labels[i]
        r['formula_sentiment'] = '0.25*h_norm + 0.25*p_norm + 0.20*z_norm + 0.15*u_norm + 0.15*s_norm (å„å› å­60æ—¥æ»šåŠ¨min-maxæ ‡å‡†åŒ–)'
        r['formula_cycle_label'] = 'å†°ç‚¹(<20)|å›æš–(<35ä¸”ä¸Šå‡)|åŠ é€Ÿ(>60ä¸”ä¸Šå‡)|åˆ†æ­§(>50ä¸”ä¸‹é™)|é€€æ½®(<40ä»>45è¿é™)|éœ‡è¡(å…¶ä»–)'


# â•â•â• è¾“å‡º JSONï¼ˆæ ¼å¼ä¸å˜ï¼‰â•â•â•

def build_json(full_rows):
    """ä»å®Œæ•´è¡Œåˆ—è¡¨ç”Ÿæˆ JSONï¼ˆæ ¼å¼ä¸åŸç‰ˆä¸€è‡´ï¼‰"""
    daily = []
    for r in full_rows:
        daily.append({
            'date': r['date'],
            'up_count': r['up_count'],
            'down_count': r['down_count'],
            'zha_count': r['zha_count'],
            'max_height': r['max_height'],
            'lianban_count': r['lianban_count'],
            'shouban_count': r['shouban_count'],
            'promotion_rate': r['promotion_rate'],
            'rate_1to2': r['rate_1to2'],
            'zha_rate': r['zha_rate'],
            'ud_ratio': r['ud_ratio'],
            'seal_quality': r['seal_quality'],
            'sentiment': r['sentiment'],
            'h_norm': r['h_norm'],
            'p_norm': r['p_norm'],
            'z_norm': r['z_norm'],
            'u_norm': r['u_norm'],
            's_norm': r['s_norm'],
            'cycle_label': r['cycle_label'],
        })

    output = {
        'meta': {
            'generated': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
            'date_range': f"{daily[0]['date']} ~ {daily[-1]['date']}",
            'count': len(daily),
            'weights': {'height': 0.25, 'promotion': 0.25, 'anti_zha': 0.20,
                       'ud_ratio': 0.15, 'seal_quality': 0.15}
        },
        'daily': daily
    }

    with open(OUTPUT_JSON, 'w', encoding='utf-8') as f:
        json.dump(output, f, ensure_ascii=False, indent=2)

    fsize = os.path.getsize(OUTPUT_JSON) / 1024
    log(f"  JSON: {OUTPUT_JSON} ({fsize:.0f} KB)")


# â•â•â• ä¸»æµç¨‹ â•â•â•

def main():
    log("=" * 50)
    log("å¼ºåŠ¿è‚¡æƒ…ç»ªæŒ‡æ ‡ï¼ˆCSVå¢é‡æ¨¡å¼ï¼‰")
    log("=" * 50)

    # 0. é¦–æ¬¡è¿ç§»
    log("\n[0] æ£€æŸ¥CSV / è¿ç§»...")
    migrate_from_json()

    # 1. è·å–äº¤æ˜“æ—¥
    log("\n[1] è·å–äº¤æ˜“æ—¥...")
    dates = get_trade_dates(LOOKBACK_DAYS)

    if not dates:
        log("  âš ï¸ Tushare è¿ä¸ä¸Šï¼Œä½¿ç”¨å·²æœ‰CSVæ•°æ®")
        raw_rows = read_raw_csv()
        if not raw_rows:
            log("  ERROR: æ— äº¤æ˜“æ—¥ä¸”æ— CSVæ•°æ®")
            sys.exit(1)
        dates = [r['date'] for r in raw_rows]
        log(f"  ä»CSVæ¢å¤: {len(dates)} å¤©: {dates[0]} ~ {dates[-1]}")
        full_rows = compute_all_metrics(raw_rows)
        write_csv(FULL_CSV, FULL_HEADERS, full_rows)
        build_json(full_rows)
        return

    log(f"  {len(dates)} ä¸ªäº¤æ˜“æ—¥: {dates[0]} ~ {dates[-1]}")

    # 2. æ‰¾å‡ºéœ€è¦å¢é‡æ‹‰å–çš„æ—¥æœŸ
    existing_dates = read_csv_dates(RAW_CSV)
    new_dates = sorted(set(dates) - existing_dates)

    if not new_dates:
        log(f"\n  æ‰€æœ‰ {len(dates)} å¤©æ•°æ®å·²åœ¨CSVä¸­ï¼Œæ— éœ€æ‹‰å–")
    else:
        log(f"\n  éœ€è¦å¢é‡æ‹‰å–: {len(new_dates)} å¤© ({new_dates[0]} ~ {new_dates[-1]})")

    # 3. å¢é‡æ‹‰å–æ–°æ—¥æœŸçš„åŸºç¡€æ•°æ®
    if new_dates:
        log("\n[2] æ‹‰å–æ–°æ—¥æœŸæ•°æ®...")
        new_raw_rows = []
        for i, dt in enumerate(new_dates):
            cached = os.path.exists(os.path.join(CACHE_DIR, f'{dt}.json'))
            tag = 'ğŸ“¦' if cached else 'ğŸŒ'
            log(f"  [{i+1}/{len(new_dates)}] {dt} {tag}")
            row = compute_raw_day(dt)
            new_raw_rows.append(row)
            log(f"    U={row['up_count']} D={row['down_count']} Z={row['zha_count']} H={row['max_height']}")

        # è¿½åŠ åˆ° raw CSV
        append_csv(RAW_CSV, RAW_HEADERS, new_raw_rows)
        log(f"  æ–°å¢ {len(new_raw_rows)} è¡Œåˆ° momentum_raw.csv")
    else:
        log("\n[2] è·³è¿‡æ‹‰å–ï¼ˆæ•°æ®å·²å®Œæ•´ï¼‰")

    # 4. é‡æ–°è¯»å–å…¨é‡ raw æ•°æ®ï¼Œè®¡ç®—æ‰€æœ‰æŒ‡æ ‡
    log("\n[3] è®¡ç®—æŒ‡æ ‡...")
    raw_rows = read_raw_csv()
    # åªä¿ç•™ dates èŒƒå›´å†…çš„
    date_set = set(dates)
    raw_rows = [r for r in raw_rows if r['date'] in date_set]
    raw_rows.sort(key=lambda x: x['date'])

    full_rows = compute_all_metrics(raw_rows)

    # 5. å†™å®Œæ•´ CSV
    write_csv(FULL_CSV, FULL_HEADERS, full_rows)
    log(f"  momentum_sentiment.csv: {len(full_rows)} è¡Œ")

    # 6. è¾“å‡º JSON
    log("\n[4] è¾“å‡º JSON...")
    build_json(full_rows)

    latest = full_rows[-1]
    log(f"\nâœ… å®Œæˆ: {len(full_rows)} å¤©")
    log(f"   æœ€æ–°: {latest['date']} æƒ…ç»ª={latest['sentiment']} å‘¨æœŸ={latest['cycle_label']}")
    log(f"   CSV: momentum_raw.csv + momentum_sentiment.csv")


if __name__ == '__main__':
    main()
